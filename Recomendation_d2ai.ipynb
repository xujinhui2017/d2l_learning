{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T14:45:31.280355Z",
     "start_time": "2020-07-28T14:45:23.342768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.16.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/c9/3526a357b6c35e5529158fbcfac1bb3adc8827e8809a6d254019d326d1cc/numpy-1.16.4-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Found existing installation: numpy 1.18.2\n",
      "    Uninstalling numpy-1.18.2:\n",
      "      Successfully uninstalled numpy-1.18.2\n",
      "Successfully installed numpy-1.16.4\n",
      "\u001b[33mYou are using pip version 18.0, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ndcg_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-609-bd0f2b7e7693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install numpy==1.16.4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndcg_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ndcg_score'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.sparse import rand as sprand\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "!pip install numpy==1.16.4\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T14:47:58.462394Z",
     "start_time": "2020-07-28T14:47:58.448092Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ndcg_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-613-f6b16a10fb05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndcg_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ndcg_score'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:41:29.056530Z",
     "start_time": "2020-07-21T13:41:29.046910Z"
    }
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def read_data_ml100k():\n",
    "\n",
    "    names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    data = pd.read_csv('u.data', '\\t', names=names,\n",
    "                       engine='python')\n",
    "    num_users = data.user_id.unique().shape[0]\n",
    "    num_items = data.item_id.unique().shape[0]\n",
    "    return data, num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:41:30.303167Z",
     "start_time": "2020-07-21T13:41:29.389934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users: 943, number of items: 1682\n",
      "matrix sparsity: 0.936953\n",
      "   user_id  item_id  rating  timestamp\n",
      "0      196      242       3  881250949\n",
      "1      186      302       3  891717742\n",
      "2       22      377       1  878887116\n",
      "3      244       51       2  880606923\n",
      "4      166      346       1  886397596\n"
     ]
    }
   ],
   "source": [
    "data, num_users, num_items = read_data_ml100k()\n",
    "sparsity = 1 - len(data) / (num_users * num_items)\n",
    "print(f'number of users: {num_users}, number of items: {num_items}')\n",
    "print(f'matrix sparsity: {sparsity:f}')\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:41:30.491650Z",
     "start_time": "2020-07-21T13:41:30.305312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130d15470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.hist(data['rating'], bins=5, ec='black')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Ratings in MovieLens 100K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:41:31.145732Z",
     "start_time": "2020-07-21T13:41:31.110721Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data_ml100k(data, num_users, num_items,\n",
    "                      split_mode='random', test_ratio=0.1):\n",
    "    \"\"\"Split the dataset in random mode or seq-aware mode.\"\"\"\n",
    "    if split_mode == 'seq-aware':\n",
    "        train_items, test_items, train_list = {}, {}, []\n",
    "        for line in data.itertuples():\n",
    "            u, i, rating, time = line[1], line[2], line[3], line[4]\n",
    "            train_items.setdefault(u, []).append((u, i, rating, time))\n",
    "            if u not in test_items or test_items[u][-1] < time:\n",
    "                test_items[u] = (i, rating, time)\n",
    "        for u in range(1, num_users + 1):\n",
    "            train_list.extend(sorted(train_items[u], key=lambda k: k[3]))\n",
    "        test_data = [(key, *value) for key, value in test_items.items()]\n",
    "        train_data = [item for item in train_list if item not in test_data]\n",
    "        train_data = pd.DataFrame(train_data)\n",
    "        test_data = pd.DataFrame(test_data)\n",
    "    else:\n",
    "        mask = [True if x == 1 else False for x in np.random.uniform(\n",
    "            0, 1, (len(data))) < 1 - test_ratio]\n",
    "        neg_mask = [not x for x in mask]\n",
    "        train_data, test_data = data[mask], data[neg_mask]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:41:32.214033Z",
     "start_time": "2020-07-21T13:41:32.197531Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_ml100k(data, num_users, num_items, feedback='explicit'):\n",
    "    users, items, scores = [], [], []\n",
    "    inter = np.zeros((num_items, num_users)) if feedback == 'explicit' else {}\n",
    "    for line in data.itertuples():\n",
    "        user_index, item_index = int(line[1] - 1), int(line[2] - 1)\n",
    "        score = int(line[3]) if feedback == 'explicit' else 1\n",
    "        users.append(user_index)\n",
    "        items.append(item_index)\n",
    "        scores.append(score)\n",
    "        if feedback == 'implicit':\n",
    "            inter.setdefault(user_index, []).append(item_index)\n",
    "        else:\n",
    "            inter[item_index, user_index] = score\n",
    "    return users, items, scores, inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:58:28.517883Z",
     "start_time": "2020-07-21T13:58:28.386411Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_original_data(filename: str):\n",
    "    train_dict = dict()\n",
    "    test_dict = dict()\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    max_user_id, min_user_id, max_item_id, min_item_id = [10] * 4\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        for idx, line in enumerate(txt_file):\n",
    "            # if idx > 50:\n",
    "            #     break\n",
    "            user_id, item_id, rating, times = line.strip().split('\\t')\n",
    "            user_id = int(user_id)\n",
    "            item_id = int(item_id)\n",
    "            rating = int(rating)\n",
    "\n",
    "            if user_id > max_user_id:\n",
    "                max_user_id = user_id\n",
    "            elif user_id < min_user_id:\n",
    "                min_user_id = user_id\n",
    "\n",
    "            if item_id > max_item_id:\n",
    "                max_item_id = item_id\n",
    "            elif item_id < min_item_id:\n",
    "                min_item_id = item_id\n",
    "\n",
    "            train_dict.setdefault(user_id, []).append((item_id, rating))\n",
    "            if user_id not in test_dict or times > test_dict[user_id][-1]:\n",
    "                test_dict[user_id] = (item_id, rating, times)\n",
    "\n",
    "    for user_id in train_dict:\n",
    "        for info in train_dict[user_id]:\n",
    "            item_id = info[0]\n",
    "            if item_id != test_dict[user_id][0]:\n",
    "                train_list.append((user_id, item_id, info[1]))\n",
    "            else:\n",
    "                test_list.append((user_id, item_id, info[1]))\n",
    "    return train_list, test_list, [min_user_id, max_user_id], [min_item_id, max_item_id]\n",
    "\n",
    "\n",
    "def generate_matrix(n_items: int, n_users: int, data: list):\n",
    "    result = [[0] * n_users] * n_items\n",
    "    for element in data:\n",
    "        user_id, item_id, rating = element\n",
    "        result[item_id - 1][user_id - 1] = rating\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_format(target_list: list):\n",
    "    return \"\\t\".join([str(i) for i in target_list]) + \"\\n\"\n",
    "\n",
    "\n",
    "def evaluate(test_info: list, predict_matrix: list, filename: str):\n",
    "    mse = 0\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "        for element in test_info:\n",
    "            user_id, item_id, rating = element\n",
    "            predict_value = predict_matrix[item_id - 1][user_id - 1]\n",
    "            mse += (predict_value - rating) ** 2\n",
    "            txt_file.write(write_format(target_list=[user_id, item_id, rating, predict_value]))\n",
    "\n",
    "    mse /= len(test_data)\n",
    "    mse = np.sqrt(mse)\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R interaction matrix \\n\n",
    "P: measure the extent of interest the use has in items' corresponding characteristics' \\n\n",
    "R_ui = p_u q_i + b_u + b_i \\n\n",
    "train using l2 loss + ridge regression penality \\n\n",
    "matrix evaluation is RMSE loss \\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T13:47:14.267654Z",
     "start_time": "2020-07-14T13:47:13.373263Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:36:22.373247Z",
     "start_time": "2020-07-21T14:36:22.351640Z"
    }
   },
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(n_users, \n",
    "                                               n_factors\n",
    "                                               )\n",
    "        self.item_factors = torch.nn.Embedding(n_items, \n",
    "                                               n_factors\n",
    "                                               \n",
    "                                               )\n",
    "        self.user_bias = torch.nn.Embedding(n_users, 1)\n",
    "        self.item_bias = torch.nn.Embedding(n_items, 1)\n",
    "        torch.nn.init.normal_(self.user_factors.weight, 0.1)\n",
    "        torch.nn.init.normal_(self.item_factors.weight, 0.1)\n",
    "        self.user_bias.weight.data.fill_(0.)\n",
    "        self.item_bias.weight.data.fill_(0.)\n",
    "        \n",
    "    def forward(self, user, item): #squeeze remove dimension equals to 1\n",
    "        #print((self.user_factors(user) * self.item_factors(item)).sum(1).shape)\n",
    "        #print(self.user_bias(user).shape)\n",
    "        return torch.nn.functional.relu((self.user_factors(user) * self.item_factors(item)).sum(1) + self.user_bias(user).squeeze() + self.item_bias(item).squeeze())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:36:22.884368Z",
     "start_time": "2020-07-21T14:36:22.665537Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate data\n",
    "num_users, num_items = 943, 1682\n",
    "train, test = split_data_ml100k(data, num_users, num_items,\n",
    "                      split_mode='random', test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:36:22.893226Z",
     "start_time": "2020-07-21T14:36:22.886351Z"
    }
   },
   "outputs": [],
   "source": [
    "#evaluation method\n",
    "def evaluator(model, test):\n",
    "    rating = torch.FloatTensor([test['rating'].values]).squeeze()\n",
    "    row = torch.LongTensor([test['user_id'].values]).squeeze()\n",
    "    col = torch.LongTensor([test['item_id'].values]).squeeze()\n",
    "    return torch.sqrt(((model(row, col) - rating)**2).sum()/len(rating))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:45:49.884099Z",
     "start_time": "2020-07-21T14:36:23.095177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1235, grad_fn=<SqrtBackward>) tensor(1557.1904, grad_fn=<AddBackward0>) tensor(3.1311, grad_fn=<SqrtBackward>)\n",
      "tensor(3.1038, grad_fn=<SqrtBackward>) tensor(1526.6769, grad_fn=<AddBackward0>) tensor(3.0955, grad_fn=<SqrtBackward>)\n",
      "tensor(3.0839, grad_fn=<SqrtBackward>) tensor(1489.7649, grad_fn=<AddBackward0>) tensor(3.0595, grad_fn=<SqrtBackward>)\n",
      "tensor(3.0638, grad_fn=<SqrtBackward>) tensor(1455.4121, grad_fn=<AddBackward0>) tensor(3.0242, grad_fn=<SqrtBackward>)\n",
      "tensor(3.0437, grad_fn=<SqrtBackward>) tensor(1420.6229, grad_fn=<AddBackward0>) tensor(2.9896, grad_fn=<SqrtBackward>)\n",
      "tensor(3.0234, grad_fn=<SqrtBackward>) tensor(1388.5786, grad_fn=<AddBackward0>) tensor(2.9557, grad_fn=<SqrtBackward>)\n",
      "tensor(3.0028, grad_fn=<SqrtBackward>) tensor(1356.2891, grad_fn=<AddBackward0>) tensor(2.9225, grad_fn=<SqrtBackward>)\n",
      "tensor(2.9820, grad_fn=<SqrtBackward>) tensor(1326.1670, grad_fn=<AddBackward0>) tensor(2.8899, grad_fn=<SqrtBackward>)\n",
      "tensor(2.9609, grad_fn=<SqrtBackward>) tensor(1296.8573, grad_fn=<AddBackward0>) tensor(2.8577, grad_fn=<SqrtBackward>)\n",
      "tensor(2.9396, grad_fn=<SqrtBackward>) tensor(1268.0131, grad_fn=<AddBackward0>) tensor(2.8258, grad_fn=<SqrtBackward>)\n",
      "tensor(2.9176, grad_fn=<SqrtBackward>) tensor(1240.0801, grad_fn=<AddBackward0>) tensor(2.7941, grad_fn=<SqrtBackward>)\n",
      "tensor(2.8951, grad_fn=<SqrtBackward>) tensor(1212.6632, grad_fn=<AddBackward0>) tensor(2.7624, grad_fn=<SqrtBackward>)\n",
      "tensor(2.8721, grad_fn=<SqrtBackward>) tensor(1185.9357, grad_fn=<AddBackward0>) tensor(2.7309, grad_fn=<SqrtBackward>)\n",
      "tensor(2.8483, grad_fn=<SqrtBackward>) tensor(1156.8456, grad_fn=<AddBackward0>) tensor(2.6991, grad_fn=<SqrtBackward>)\n",
      "tensor(2.8235, grad_fn=<SqrtBackward>) tensor(1131.2811, grad_fn=<AddBackward0>) tensor(2.6669, grad_fn=<SqrtBackward>)\n",
      "tensor(2.7979, grad_fn=<SqrtBackward>) tensor(1102.8186, grad_fn=<AddBackward0>) tensor(2.6345, grad_fn=<SqrtBackward>)\n",
      "tensor(2.7713, grad_fn=<SqrtBackward>) tensor(1076.8062, grad_fn=<AddBackward0>) tensor(2.6017, grad_fn=<SqrtBackward>)\n",
      "tensor(2.7437, grad_fn=<SqrtBackward>) tensor(1049.4327, grad_fn=<AddBackward0>) tensor(2.5687, grad_fn=<SqrtBackward>)\n",
      "tensor(2.7149, grad_fn=<SqrtBackward>) tensor(1023.0238, grad_fn=<AddBackward0>) tensor(2.5353, grad_fn=<SqrtBackward>)\n",
      "tensor(2.6851, grad_fn=<SqrtBackward>) tensor(997.0765, grad_fn=<AddBackward0>) tensor(2.5018, grad_fn=<SqrtBackward>)\n",
      "tensor(2.6547, grad_fn=<SqrtBackward>) tensor(969.3572, grad_fn=<AddBackward0>) tensor(2.4680, grad_fn=<SqrtBackward>)\n",
      "tensor(2.6235, grad_fn=<SqrtBackward>) tensor(942.3109, grad_fn=<AddBackward0>) tensor(2.4341, grad_fn=<SqrtBackward>)\n",
      "tensor(2.5914, grad_fn=<SqrtBackward>) tensor(917.3665, grad_fn=<AddBackward0>) tensor(2.3999, grad_fn=<SqrtBackward>)\n",
      "tensor(2.5587, grad_fn=<SqrtBackward>) tensor(892.0272, grad_fn=<AddBackward0>) tensor(2.3652, grad_fn=<SqrtBackward>)\n",
      "tensor(2.5254, grad_fn=<SqrtBackward>) tensor(865.2130, grad_fn=<AddBackward0>) tensor(2.3304, grad_fn=<SqrtBackward>)\n",
      "tensor(2.4914, grad_fn=<SqrtBackward>) tensor(838.9993, grad_fn=<AddBackward0>) tensor(2.2952, grad_fn=<SqrtBackward>)\n",
      "tensor(2.4562, grad_fn=<SqrtBackward>) tensor(813.4288, grad_fn=<AddBackward0>) tensor(2.2596, grad_fn=<SqrtBackward>)\n",
      "tensor(2.4204, grad_fn=<SqrtBackward>) tensor(789.3691, grad_fn=<AddBackward0>) tensor(2.2238, grad_fn=<SqrtBackward>)\n",
      "tensor(2.3841, grad_fn=<SqrtBackward>) tensor(764.3696, grad_fn=<AddBackward0>) tensor(2.1878, grad_fn=<SqrtBackward>)\n",
      "tensor(2.3472, grad_fn=<SqrtBackward>) tensor(738.9825, grad_fn=<AddBackward0>) tensor(2.1514, grad_fn=<SqrtBackward>)\n",
      "tensor(2.3097, grad_fn=<SqrtBackward>) tensor(714.0245, grad_fn=<AddBackward0>) tensor(2.1148, grad_fn=<SqrtBackward>)\n",
      "tensor(2.2723, grad_fn=<SqrtBackward>) tensor(690.7130, grad_fn=<AddBackward0>) tensor(2.0779, grad_fn=<SqrtBackward>)\n",
      "tensor(2.2346, grad_fn=<SqrtBackward>) tensor(665.2375, grad_fn=<AddBackward0>) tensor(2.0411, grad_fn=<SqrtBackward>)\n",
      "tensor(2.1968, grad_fn=<SqrtBackward>) tensor(642.6505, grad_fn=<AddBackward0>) tensor(2.0044, grad_fn=<SqrtBackward>)\n",
      "tensor(2.1592, grad_fn=<SqrtBackward>) tensor(619.1694, grad_fn=<AddBackward0>) tensor(1.9678, grad_fn=<SqrtBackward>)\n",
      "tensor(2.1217, grad_fn=<SqrtBackward>) tensor(596.3593, grad_fn=<AddBackward0>) tensor(1.9316, grad_fn=<SqrtBackward>)\n",
      "tensor(2.0846, grad_fn=<SqrtBackward>) tensor(574.4733, grad_fn=<AddBackward0>) tensor(1.8955, grad_fn=<SqrtBackward>)\n",
      "tensor(2.0476, grad_fn=<SqrtBackward>) tensor(552.9677, grad_fn=<AddBackward0>) tensor(1.8598, grad_fn=<SqrtBackward>)\n",
      "tensor(2.0109, grad_fn=<SqrtBackward>) tensor(532.3586, grad_fn=<AddBackward0>) tensor(1.8246, grad_fn=<SqrtBackward>)\n",
      "tensor(1.9749, grad_fn=<SqrtBackward>) tensor(512.9445, grad_fn=<AddBackward0>) tensor(1.7898, grad_fn=<SqrtBackward>)\n",
      "tensor(1.9396, grad_fn=<SqrtBackward>) tensor(493.7789, grad_fn=<AddBackward0>) tensor(1.7556, grad_fn=<SqrtBackward>)\n",
      "tensor(1.9050, grad_fn=<SqrtBackward>) tensor(474.8106, grad_fn=<AddBackward0>) tensor(1.7223, grad_fn=<SqrtBackward>)\n",
      "tensor(1.8713, grad_fn=<SqrtBackward>) tensor(456.5820, grad_fn=<AddBackward0>) tensor(1.6898, grad_fn=<SqrtBackward>)\n",
      "tensor(1.8384, grad_fn=<SqrtBackward>) tensor(440.2884, grad_fn=<AddBackward0>) tensor(1.6582, grad_fn=<SqrtBackward>)\n",
      "tensor(1.8064, grad_fn=<SqrtBackward>) tensor(423.9980, grad_fn=<AddBackward0>) tensor(1.6276, grad_fn=<SqrtBackward>)\n",
      "tensor(1.7757, grad_fn=<SqrtBackward>) tensor(408.1350, grad_fn=<AddBackward0>) tensor(1.5979, grad_fn=<SqrtBackward>)\n",
      "tensor(1.7461, grad_fn=<SqrtBackward>) tensor(393.3169, grad_fn=<AddBackward0>) tensor(1.5692, grad_fn=<SqrtBackward>)\n",
      "tensor(1.7175, grad_fn=<SqrtBackward>) tensor(379.6288, grad_fn=<AddBackward0>) tensor(1.5415, grad_fn=<SqrtBackward>)\n",
      "tensor(1.6899, grad_fn=<SqrtBackward>) tensor(366.2236, grad_fn=<AddBackward0>) tensor(1.5149, grad_fn=<SqrtBackward>)\n",
      "tensor(1.6633, grad_fn=<SqrtBackward>) tensor(354.5433, grad_fn=<AddBackward0>) tensor(1.4894, grad_fn=<SqrtBackward>)\n",
      "tensor(1.6376, grad_fn=<SqrtBackward>) tensor(342.6213, grad_fn=<AddBackward0>) tensor(1.4648, grad_fn=<SqrtBackward>)\n",
      "tensor(1.6131, grad_fn=<SqrtBackward>) tensor(331.1185, grad_fn=<AddBackward0>) tensor(1.4412, grad_fn=<SqrtBackward>)\n",
      "tensor(1.5893, grad_fn=<SqrtBackward>) tensor(320.9913, grad_fn=<AddBackward0>) tensor(1.4187, grad_fn=<SqrtBackward>)\n",
      "tensor(1.5665, grad_fn=<SqrtBackward>) tensor(311.1288, grad_fn=<AddBackward0>) tensor(1.3973, grad_fn=<SqrtBackward>)\n",
      "tensor(1.5445, grad_fn=<SqrtBackward>) tensor(301.8831, grad_fn=<AddBackward0>) tensor(1.3767, grad_fn=<SqrtBackward>)\n",
      "tensor(1.5234, grad_fn=<SqrtBackward>) tensor(293.1843, grad_fn=<AddBackward0>) tensor(1.3572, grad_fn=<SqrtBackward>)\n",
      "tensor(1.5031, grad_fn=<SqrtBackward>) tensor(285.2243, grad_fn=<AddBackward0>) tensor(1.3387, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4836, grad_fn=<SqrtBackward>) tensor(277.7147, grad_fn=<AddBackward0>) tensor(1.3212, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4650, grad_fn=<SqrtBackward>) tensor(270.2799, grad_fn=<AddBackward0>) tensor(1.3046, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4475, grad_fn=<SqrtBackward>) tensor(263.8246, grad_fn=<AddBackward0>) tensor(1.2889, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4307, grad_fn=<SqrtBackward>) tensor(257.6270, grad_fn=<AddBackward0>) tensor(1.2742, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4148, grad_fn=<SqrtBackward>) tensor(251.8820, grad_fn=<AddBackward0>) tensor(1.2602, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3997, grad_fn=<SqrtBackward>) tensor(246.4404, grad_fn=<AddBackward0>) tensor(1.2471, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3855, grad_fn=<SqrtBackward>) tensor(241.4751, grad_fn=<AddBackward0>) tensor(1.2348, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3721, grad_fn=<SqrtBackward>) tensor(236.9474, grad_fn=<AddBackward0>) tensor(1.2233, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3594, grad_fn=<SqrtBackward>) tensor(232.8784, grad_fn=<AddBackward0>) tensor(1.2125, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3476, grad_fn=<SqrtBackward>) tensor(228.6596, grad_fn=<AddBackward0>) tensor(1.2026, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3365, grad_fn=<SqrtBackward>) tensor(225.2533, grad_fn=<AddBackward0>) tensor(1.1933, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3262, grad_fn=<SqrtBackward>) tensor(221.6095, grad_fn=<AddBackward0>) tensor(1.1847, grad_fn=<SqrtBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3165, grad_fn=<SqrtBackward>) tensor(218.5892, grad_fn=<AddBackward0>) tensor(1.1767, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3074, grad_fn=<SqrtBackward>) tensor(216.1216, grad_fn=<AddBackward0>) tensor(1.1694, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2989, grad_fn=<SqrtBackward>) tensor(213.4506, grad_fn=<AddBackward0>) tensor(1.1626, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2910, grad_fn=<SqrtBackward>) tensor(210.7450, grad_fn=<AddBackward0>) tensor(1.1563, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2837, grad_fn=<SqrtBackward>) tensor(208.4991, grad_fn=<AddBackward0>) tensor(1.1506, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2769, grad_fn=<SqrtBackward>) tensor(207.0265, grad_fn=<AddBackward0>) tensor(1.1452, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2706, grad_fn=<SqrtBackward>) tensor(204.7521, grad_fn=<AddBackward0>) tensor(1.1403, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2647, grad_fn=<SqrtBackward>) tensor(202.9427, grad_fn=<AddBackward0>) tensor(1.1357, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2592, grad_fn=<SqrtBackward>) tensor(201.5611, grad_fn=<AddBackward0>) tensor(1.1315, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2540, grad_fn=<SqrtBackward>) tensor(200.2293, grad_fn=<AddBackward0>) tensor(1.1275, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2492, grad_fn=<SqrtBackward>) tensor(198.8415, grad_fn=<AddBackward0>) tensor(1.1238, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2447, grad_fn=<SqrtBackward>) tensor(197.5511, grad_fn=<AddBackward0>) tensor(1.1204, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2406, grad_fn=<SqrtBackward>) tensor(196.2196, grad_fn=<AddBackward0>) tensor(1.1172, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2368, grad_fn=<SqrtBackward>) tensor(195.3473, grad_fn=<AddBackward0>) tensor(1.1143, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2333, grad_fn=<SqrtBackward>) tensor(194.4921, grad_fn=<AddBackward0>) tensor(1.1117, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2302, grad_fn=<SqrtBackward>) tensor(193.3410, grad_fn=<AddBackward0>) tensor(1.1093, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2273, grad_fn=<SqrtBackward>) tensor(192.7557, grad_fn=<AddBackward0>) tensor(1.1073, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2248, grad_fn=<SqrtBackward>) tensor(192.1806, grad_fn=<AddBackward0>) tensor(1.1056, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2226, grad_fn=<SqrtBackward>) tensor(191.7165, grad_fn=<AddBackward0>) tensor(1.1042, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2207, grad_fn=<SqrtBackward>) tensor(191.1511, grad_fn=<AddBackward0>) tensor(1.1031, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2190, grad_fn=<SqrtBackward>) tensor(190.7249, grad_fn=<AddBackward0>) tensor(1.1022, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2176, grad_fn=<SqrtBackward>) tensor(190.4213, grad_fn=<AddBackward0>) tensor(1.1017, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2165, grad_fn=<SqrtBackward>) tensor(190.6001, grad_fn=<AddBackward0>) tensor(1.1014, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2156, grad_fn=<SqrtBackward>) tensor(190.4745, grad_fn=<AddBackward0>) tensor(1.1014, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2151, grad_fn=<SqrtBackward>) tensor(190.1824, grad_fn=<AddBackward0>) tensor(1.1016, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2148, grad_fn=<SqrtBackward>) tensor(190.4751, grad_fn=<AddBackward0>) tensor(1.1020, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2148, grad_fn=<SqrtBackward>) tensor(190.4168, grad_fn=<AddBackward0>) tensor(1.1027, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2150, grad_fn=<SqrtBackward>) tensor(191.0249, grad_fn=<AddBackward0>) tensor(1.1035, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2154, grad_fn=<SqrtBackward>) tensor(191.1266, grad_fn=<AddBackward0>) tensor(1.1045, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2160, grad_fn=<SqrtBackward>) tensor(191.5432, grad_fn=<AddBackward0>) tensor(1.1057, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2169, grad_fn=<SqrtBackward>) tensor(192.2390, grad_fn=<AddBackward0>) tensor(1.1071, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2179, grad_fn=<SqrtBackward>) tensor(192.4520, grad_fn=<AddBackward0>) tensor(1.1086, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2191, grad_fn=<SqrtBackward>) tensor(193.2787, grad_fn=<AddBackward0>) tensor(1.1102, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2205, grad_fn=<SqrtBackward>) tensor(193.7060, grad_fn=<AddBackward0>) tensor(1.1120, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2221, grad_fn=<SqrtBackward>) tensor(194.3655, grad_fn=<AddBackward0>) tensor(1.1139, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2239, grad_fn=<SqrtBackward>) tensor(194.8056, grad_fn=<AddBackward0>) tensor(1.1159, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2258, grad_fn=<SqrtBackward>) tensor(195.6628, grad_fn=<AddBackward0>) tensor(1.1181, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2278, grad_fn=<SqrtBackward>) tensor(196.3453, grad_fn=<AddBackward0>) tensor(1.1203, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2301, grad_fn=<SqrtBackward>) tensor(197.7485, grad_fn=<AddBackward0>) tensor(1.1227, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2325, grad_fn=<SqrtBackward>) tensor(197.9845, grad_fn=<AddBackward0>) tensor(1.1252, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2351, grad_fn=<SqrtBackward>) tensor(199.1107, grad_fn=<AddBackward0>) tensor(1.1278, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2378, grad_fn=<SqrtBackward>) tensor(200.0814, grad_fn=<AddBackward0>) tensor(1.1305, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2408, grad_fn=<SqrtBackward>) tensor(201.1703, grad_fn=<AddBackward0>) tensor(1.1334, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2439, grad_fn=<SqrtBackward>) tensor(202.0978, grad_fn=<AddBackward0>) tensor(1.1364, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2473, grad_fn=<SqrtBackward>) tensor(203.3027, grad_fn=<AddBackward0>) tensor(1.1396, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2509, grad_fn=<SqrtBackward>) tensor(204.2884, grad_fn=<AddBackward0>) tensor(1.1429, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2547, grad_fn=<SqrtBackward>) tensor(205.4705, grad_fn=<AddBackward0>) tensor(1.1464, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2586, grad_fn=<SqrtBackward>) tensor(206.6961, grad_fn=<AddBackward0>) tensor(1.1500, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2628, grad_fn=<SqrtBackward>) tensor(208.1675, grad_fn=<AddBackward0>) tensor(1.1537, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2670, grad_fn=<SqrtBackward>) tensor(209.6269, grad_fn=<AddBackward0>) tensor(1.1575, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2714, grad_fn=<SqrtBackward>) tensor(211.0788, grad_fn=<AddBackward0>) tensor(1.1614, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2759, grad_fn=<SqrtBackward>) tensor(212.2832, grad_fn=<AddBackward0>) tensor(1.1654, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2805, grad_fn=<SqrtBackward>) tensor(213.8311, grad_fn=<AddBackward0>) tensor(1.1694, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2852, grad_fn=<SqrtBackward>) tensor(215.5014, grad_fn=<AddBackward0>) tensor(1.1735, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2899, grad_fn=<SqrtBackward>) tensor(216.5270, grad_fn=<AddBackward0>) tensor(1.1776, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2947, grad_fn=<SqrtBackward>) tensor(218.3707, grad_fn=<AddBackward0>) tensor(1.1817, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2995, grad_fn=<SqrtBackward>) tensor(220.3175, grad_fn=<AddBackward0>) tensor(1.1857, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3044, grad_fn=<SqrtBackward>) tensor(221.2155, grad_fn=<AddBackward0>) tensor(1.1898, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3093, grad_fn=<SqrtBackward>) tensor(223.0963, grad_fn=<AddBackward0>) tensor(1.1938, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3142, grad_fn=<SqrtBackward>) tensor(224.4594, grad_fn=<AddBackward0>) tensor(1.1977, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3192, grad_fn=<SqrtBackward>) tensor(226.1031, grad_fn=<AddBackward0>) tensor(1.2017, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3242, grad_fn=<SqrtBackward>) tensor(227.4383, grad_fn=<AddBackward0>) tensor(1.2056, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3292, grad_fn=<SqrtBackward>) tensor(228.6176, grad_fn=<AddBackward0>) tensor(1.2095, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3343, grad_fn=<SqrtBackward>) tensor(230.3761, grad_fn=<AddBackward0>) tensor(1.2133, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3393, grad_fn=<SqrtBackward>) tensor(231.5825, grad_fn=<AddBackward0>) tensor(1.2171, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3443, grad_fn=<SqrtBackward>) tensor(232.7965, grad_fn=<AddBackward0>) tensor(1.2209, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3493, grad_fn=<SqrtBackward>) tensor(234.4709, grad_fn=<AddBackward0>) tensor(1.2245, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3542, grad_fn=<SqrtBackward>) tensor(235.7783, grad_fn=<AddBackward0>) tensor(1.2282, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3591, grad_fn=<SqrtBackward>) tensor(237.2000, grad_fn=<AddBackward0>) tensor(1.2317, grad_fn=<SqrtBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3640, grad_fn=<SqrtBackward>) tensor(238.4856, grad_fn=<AddBackward0>) tensor(1.2352, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3687, grad_fn=<SqrtBackward>) tensor(239.9287, grad_fn=<AddBackward0>) tensor(1.2385, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3734, grad_fn=<SqrtBackward>) tensor(240.8320, grad_fn=<AddBackward0>) tensor(1.2418, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3780, grad_fn=<SqrtBackward>) tensor(242.9009, grad_fn=<AddBackward0>) tensor(1.2450, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3826, grad_fn=<SqrtBackward>) tensor(243.7868, grad_fn=<AddBackward0>) tensor(1.2482, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3870, grad_fn=<SqrtBackward>) tensor(245.2923, grad_fn=<AddBackward0>) tensor(1.2512, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3914, grad_fn=<SqrtBackward>) tensor(246.2611, grad_fn=<AddBackward0>) tensor(1.2541, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3955, grad_fn=<SqrtBackward>) tensor(247.1018, grad_fn=<AddBackward0>) tensor(1.2568, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3995, grad_fn=<SqrtBackward>) tensor(248.2689, grad_fn=<AddBackward0>) tensor(1.2594, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4034, grad_fn=<SqrtBackward>) tensor(249.3703, grad_fn=<AddBackward0>) tensor(1.2619, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4070, grad_fn=<SqrtBackward>) tensor(250.3111, grad_fn=<AddBackward0>) tensor(1.2642, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4105, grad_fn=<SqrtBackward>) tensor(250.8874, grad_fn=<AddBackward0>) tensor(1.2663, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4139, grad_fn=<SqrtBackward>) tensor(252.0426, grad_fn=<AddBackward0>) tensor(1.2683, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4170, grad_fn=<SqrtBackward>) tensor(252.2553, grad_fn=<AddBackward0>) tensor(1.2701, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4200, grad_fn=<SqrtBackward>) tensor(253.2877, grad_fn=<AddBackward0>) tensor(1.2718, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4229, grad_fn=<SqrtBackward>) tensor(253.4952, grad_fn=<AddBackward0>) tensor(1.2734, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4255, grad_fn=<SqrtBackward>) tensor(254.3504, grad_fn=<AddBackward0>) tensor(1.2748, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4280, grad_fn=<SqrtBackward>) tensor(255.0508, grad_fn=<AddBackward0>) tensor(1.2761, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4303, grad_fn=<SqrtBackward>) tensor(255.2745, grad_fn=<AddBackward0>) tensor(1.2773, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4324, grad_fn=<SqrtBackward>) tensor(256.2956, grad_fn=<AddBackward0>) tensor(1.2783, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4343, grad_fn=<SqrtBackward>) tensor(256.1555, grad_fn=<AddBackward0>) tensor(1.2792, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4361, grad_fn=<SqrtBackward>) tensor(256.7042, grad_fn=<AddBackward0>) tensor(1.2799, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4378, grad_fn=<SqrtBackward>) tensor(257.0771, grad_fn=<AddBackward0>) tensor(1.2806, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4393, grad_fn=<SqrtBackward>) tensor(257.4102, grad_fn=<AddBackward0>) tensor(1.2811, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4407, grad_fn=<SqrtBackward>) tensor(257.2494, grad_fn=<AddBackward0>) tensor(1.2814, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4420, grad_fn=<SqrtBackward>) tensor(257.5192, grad_fn=<AddBackward0>) tensor(1.2817, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4431, grad_fn=<SqrtBackward>) tensor(257.6937, grad_fn=<AddBackward0>) tensor(1.2817, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4440, grad_fn=<SqrtBackward>) tensor(257.5909, grad_fn=<AddBackward0>) tensor(1.2817, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4449, grad_fn=<SqrtBackward>) tensor(257.7372, grad_fn=<AddBackward0>) tensor(1.2815, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4456, grad_fn=<SqrtBackward>) tensor(257.3218, grad_fn=<AddBackward0>) tensor(1.2811, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4462, grad_fn=<SqrtBackward>) tensor(257.0604, grad_fn=<AddBackward0>) tensor(1.2807, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4467, grad_fn=<SqrtBackward>) tensor(256.6199, grad_fn=<AddBackward0>) tensor(1.2801, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4471, grad_fn=<SqrtBackward>) tensor(256.9872, grad_fn=<AddBackward0>) tensor(1.2795, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4473, grad_fn=<SqrtBackward>) tensor(256.4043, grad_fn=<AddBackward0>) tensor(1.2787, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4475, grad_fn=<SqrtBackward>) tensor(256.1971, grad_fn=<AddBackward0>) tensor(1.2779, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4476, grad_fn=<SqrtBackward>) tensor(255.7976, grad_fn=<AddBackward0>) tensor(1.2771, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4476, grad_fn=<SqrtBackward>) tensor(255.7888, grad_fn=<AddBackward0>) tensor(1.2763, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4476, grad_fn=<SqrtBackward>) tensor(255.2866, grad_fn=<AddBackward0>) tensor(1.2754, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4476, grad_fn=<SqrtBackward>) tensor(254.8183, grad_fn=<AddBackward0>) tensor(1.2745, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4475, grad_fn=<SqrtBackward>) tensor(254.5009, grad_fn=<AddBackward0>) tensor(1.2735, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4473, grad_fn=<SqrtBackward>) tensor(254.0155, grad_fn=<AddBackward0>) tensor(1.2726, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4471, grad_fn=<SqrtBackward>) tensor(253.3557, grad_fn=<AddBackward0>) tensor(1.2716, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4468, grad_fn=<SqrtBackward>) tensor(253.1487, grad_fn=<AddBackward0>) tensor(1.2706, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4465, grad_fn=<SqrtBackward>) tensor(253.2377, grad_fn=<AddBackward0>) tensor(1.2695, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4461, grad_fn=<SqrtBackward>) tensor(252.4114, grad_fn=<AddBackward0>) tensor(1.2685, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4456, grad_fn=<SqrtBackward>) tensor(252.3528, grad_fn=<AddBackward0>) tensor(1.2674, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4450, grad_fn=<SqrtBackward>) tensor(251.3564, grad_fn=<AddBackward0>) tensor(1.2663, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4444, grad_fn=<SqrtBackward>) tensor(251.2669, grad_fn=<AddBackward0>) tensor(1.2651, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4438, grad_fn=<SqrtBackward>) tensor(250.5412, grad_fn=<AddBackward0>) tensor(1.2638, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4431, grad_fn=<SqrtBackward>) tensor(250.4189, grad_fn=<AddBackward0>) tensor(1.2625, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4424, grad_fn=<SqrtBackward>) tensor(249.7417, grad_fn=<AddBackward0>) tensor(1.2611, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4417, grad_fn=<SqrtBackward>) tensor(249.1172, grad_fn=<AddBackward0>) tensor(1.2597, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4410, grad_fn=<SqrtBackward>) tensor(248.4433, grad_fn=<AddBackward0>) tensor(1.2582, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4405, grad_fn=<SqrtBackward>) tensor(248.0690, grad_fn=<AddBackward0>) tensor(1.2567, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4400, grad_fn=<SqrtBackward>) tensor(247.4288, grad_fn=<AddBackward0>) tensor(1.2553, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4396, grad_fn=<SqrtBackward>) tensor(246.6827, grad_fn=<AddBackward0>) tensor(1.2538, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4393, grad_fn=<SqrtBackward>) tensor(246.5412, grad_fn=<AddBackward0>) tensor(1.2523, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4391, grad_fn=<SqrtBackward>) tensor(245.4836, grad_fn=<AddBackward0>) tensor(1.2509, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4389, grad_fn=<SqrtBackward>) tensor(244.9827, grad_fn=<AddBackward0>) tensor(1.2495, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4387, grad_fn=<SqrtBackward>) tensor(244.6230, grad_fn=<AddBackward0>) tensor(1.2481, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4385, grad_fn=<SqrtBackward>) tensor(243.9768, grad_fn=<AddBackward0>) tensor(1.2468, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4384, grad_fn=<SqrtBackward>) tensor(243.6715, grad_fn=<AddBackward0>) tensor(1.2455, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4383, grad_fn=<SqrtBackward>) tensor(242.9630, grad_fn=<AddBackward0>) tensor(1.2442, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4380, grad_fn=<SqrtBackward>) tensor(242.5113, grad_fn=<AddBackward0>) tensor(1.2430, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4378, grad_fn=<SqrtBackward>) tensor(242.1449, grad_fn=<AddBackward0>) tensor(1.2418, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4377, grad_fn=<SqrtBackward>) tensor(241.7298, grad_fn=<AddBackward0>) tensor(1.2406, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4374, grad_fn=<SqrtBackward>) tensor(241.2917, grad_fn=<AddBackward0>) tensor(1.2394, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4372, grad_fn=<SqrtBackward>) tensor(240.5269, grad_fn=<AddBackward0>) tensor(1.2382, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4368, grad_fn=<SqrtBackward>) tensor(240.1837, grad_fn=<AddBackward0>) tensor(1.2370, grad_fn=<SqrtBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4364, grad_fn=<SqrtBackward>) tensor(239.6956, grad_fn=<AddBackward0>) tensor(1.2359, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4359, grad_fn=<SqrtBackward>) tensor(239.3072, grad_fn=<AddBackward0>) tensor(1.2346, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4353, grad_fn=<SqrtBackward>) tensor(238.5335, grad_fn=<AddBackward0>) tensor(1.2333, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4345, grad_fn=<SqrtBackward>) tensor(237.9101, grad_fn=<AddBackward0>) tensor(1.2319, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4336, grad_fn=<SqrtBackward>) tensor(237.5323, grad_fn=<AddBackward0>) tensor(1.2304, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4325, grad_fn=<SqrtBackward>) tensor(236.8544, grad_fn=<AddBackward0>) tensor(1.2288, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4312, grad_fn=<SqrtBackward>) tensor(236.6058, grad_fn=<AddBackward0>) tensor(1.2272, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4299, grad_fn=<SqrtBackward>) tensor(235.8109, grad_fn=<AddBackward0>) tensor(1.2254, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4283, grad_fn=<SqrtBackward>) tensor(234.9809, grad_fn=<AddBackward0>) tensor(1.2236, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4267, grad_fn=<SqrtBackward>) tensor(234.6732, grad_fn=<AddBackward0>) tensor(1.2216, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4249, grad_fn=<SqrtBackward>) tensor(233.6279, grad_fn=<AddBackward0>) tensor(1.2196, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4228, grad_fn=<SqrtBackward>) tensor(232.6253, grad_fn=<AddBackward0>) tensor(1.2174, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4207, grad_fn=<SqrtBackward>) tensor(231.9661, grad_fn=<AddBackward0>) tensor(1.2151, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4185, grad_fn=<SqrtBackward>) tensor(231.2469, grad_fn=<AddBackward0>) tensor(1.2127, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4162, grad_fn=<SqrtBackward>) tensor(230.0661, grad_fn=<AddBackward0>) tensor(1.2103, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4137, grad_fn=<SqrtBackward>) tensor(229.1439, grad_fn=<AddBackward0>) tensor(1.2078, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4112, grad_fn=<SqrtBackward>) tensor(228.0997, grad_fn=<AddBackward0>) tensor(1.2053, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4085, grad_fn=<SqrtBackward>) tensor(227.0474, grad_fn=<AddBackward0>) tensor(1.2027, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4056, grad_fn=<SqrtBackward>) tensor(226.2942, grad_fn=<AddBackward0>) tensor(1.2001, grad_fn=<SqrtBackward>)\n",
      "tensor(1.4027, grad_fn=<SqrtBackward>) tensor(225.4167, grad_fn=<AddBackward0>) tensor(1.1974, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3997, grad_fn=<SqrtBackward>) tensor(224.1849, grad_fn=<AddBackward0>) tensor(1.1946, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3967, grad_fn=<SqrtBackward>) tensor(223.0007, grad_fn=<AddBackward0>) tensor(1.1919, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3937, grad_fn=<SqrtBackward>) tensor(222.4808, grad_fn=<AddBackward0>) tensor(1.1890, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3907, grad_fn=<SqrtBackward>) tensor(221.1851, grad_fn=<AddBackward0>) tensor(1.1862, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3878, grad_fn=<SqrtBackward>) tensor(220.0424, grad_fn=<AddBackward0>) tensor(1.1834, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3850, grad_fn=<SqrtBackward>) tensor(219.0008, grad_fn=<AddBackward0>) tensor(1.1805, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3823, grad_fn=<SqrtBackward>) tensor(217.8636, grad_fn=<AddBackward0>) tensor(1.1777, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3796, grad_fn=<SqrtBackward>) tensor(216.6687, grad_fn=<AddBackward0>) tensor(1.1748, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3771, grad_fn=<SqrtBackward>) tensor(215.7885, grad_fn=<AddBackward0>) tensor(1.1720, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3746, grad_fn=<SqrtBackward>) tensor(214.9091, grad_fn=<AddBackward0>) tensor(1.1692, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3722, grad_fn=<SqrtBackward>) tensor(213.7579, grad_fn=<AddBackward0>) tensor(1.1664, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3698, grad_fn=<SqrtBackward>) tensor(212.7683, grad_fn=<AddBackward0>) tensor(1.1637, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3674, grad_fn=<SqrtBackward>) tensor(212.0685, grad_fn=<AddBackward0>) tensor(1.1610, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3650, grad_fn=<SqrtBackward>) tensor(210.7816, grad_fn=<AddBackward0>) tensor(1.1584, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3626, grad_fn=<SqrtBackward>) tensor(209.7421, grad_fn=<AddBackward0>) tensor(1.1557, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3601, grad_fn=<SqrtBackward>) tensor(208.9556, grad_fn=<AddBackward0>) tensor(1.1531, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3575, grad_fn=<SqrtBackward>) tensor(208.2753, grad_fn=<AddBackward0>) tensor(1.1506, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3549, grad_fn=<SqrtBackward>) tensor(207.0331, grad_fn=<AddBackward0>) tensor(1.1480, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3523, grad_fn=<SqrtBackward>) tensor(206.0493, grad_fn=<AddBackward0>) tensor(1.1455, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3498, grad_fn=<SqrtBackward>) tensor(205.2848, grad_fn=<AddBackward0>) tensor(1.1431, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3473, grad_fn=<SqrtBackward>) tensor(204.3907, grad_fn=<AddBackward0>) tensor(1.1406, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3450, grad_fn=<SqrtBackward>) tensor(203.6203, grad_fn=<AddBackward0>) tensor(1.1382, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3428, grad_fn=<SqrtBackward>) tensor(202.5687, grad_fn=<AddBackward0>) tensor(1.1358, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3408, grad_fn=<SqrtBackward>) tensor(201.7252, grad_fn=<AddBackward0>) tensor(1.1335, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3390, grad_fn=<SqrtBackward>) tensor(201.0885, grad_fn=<AddBackward0>) tensor(1.1312, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3374, grad_fn=<SqrtBackward>) tensor(200.1739, grad_fn=<AddBackward0>) tensor(1.1291, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3360, grad_fn=<SqrtBackward>) tensor(199.3194, grad_fn=<AddBackward0>) tensor(1.1271, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3346, grad_fn=<SqrtBackward>) tensor(199.0791, grad_fn=<AddBackward0>) tensor(1.1251, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3334, grad_fn=<SqrtBackward>) tensor(198.1218, grad_fn=<AddBackward0>) tensor(1.1233, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3323, grad_fn=<SqrtBackward>) tensor(197.4234, grad_fn=<AddBackward0>) tensor(1.1215, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3312, grad_fn=<SqrtBackward>) tensor(197.0216, grad_fn=<AddBackward0>) tensor(1.1197, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3300, grad_fn=<SqrtBackward>) tensor(196.5585, grad_fn=<AddBackward0>) tensor(1.1180, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3289, grad_fn=<SqrtBackward>) tensor(195.9274, grad_fn=<AddBackward0>) tensor(1.1163, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3278, grad_fn=<SqrtBackward>) tensor(194.7103, grad_fn=<AddBackward0>) tensor(1.1145, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3266, grad_fn=<SqrtBackward>) tensor(194.6951, grad_fn=<AddBackward0>) tensor(1.1128, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3254, grad_fn=<SqrtBackward>) tensor(193.9713, grad_fn=<AddBackward0>) tensor(1.1111, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3240, grad_fn=<SqrtBackward>) tensor(193.1740, grad_fn=<AddBackward0>) tensor(1.1094, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3226, grad_fn=<SqrtBackward>) tensor(192.8513, grad_fn=<AddBackward0>) tensor(1.1077, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3211, grad_fn=<SqrtBackward>) tensor(192.1104, grad_fn=<AddBackward0>) tensor(1.1060, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3195, grad_fn=<SqrtBackward>) tensor(191.3598, grad_fn=<AddBackward0>) tensor(1.1044, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3180, grad_fn=<SqrtBackward>) tensor(190.6186, grad_fn=<AddBackward0>) tensor(1.1028, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3164, grad_fn=<SqrtBackward>) tensor(190.1813, grad_fn=<AddBackward0>) tensor(1.1012, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3148, grad_fn=<SqrtBackward>) tensor(189.6642, grad_fn=<AddBackward0>) tensor(1.0996, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3132, grad_fn=<SqrtBackward>) tensor(189.3372, grad_fn=<AddBackward0>) tensor(1.0981, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3116, grad_fn=<SqrtBackward>) tensor(188.7879, grad_fn=<AddBackward0>) tensor(1.0965, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3100, grad_fn=<SqrtBackward>) tensor(188.4634, grad_fn=<AddBackward0>) tensor(1.0950, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3084, grad_fn=<SqrtBackward>) tensor(187.5829, grad_fn=<AddBackward0>) tensor(1.0935, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3066, grad_fn=<SqrtBackward>) tensor(187.1946, grad_fn=<AddBackward0>) tensor(1.0920, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3048, grad_fn=<SqrtBackward>) tensor(186.7962, grad_fn=<AddBackward0>) tensor(1.0905, grad_fn=<SqrtBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3030, grad_fn=<SqrtBackward>) tensor(186.2914, grad_fn=<AddBackward0>) tensor(1.0890, grad_fn=<SqrtBackward>)\n",
      "tensor(1.3012, grad_fn=<SqrtBackward>) tensor(185.6501, grad_fn=<AddBackward0>) tensor(1.0874, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2994, grad_fn=<SqrtBackward>) tensor(185.1085, grad_fn=<AddBackward0>) tensor(1.0859, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2975, grad_fn=<SqrtBackward>) tensor(184.7729, grad_fn=<AddBackward0>) tensor(1.0844, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2956, grad_fn=<SqrtBackward>) tensor(183.9917, grad_fn=<AddBackward0>) tensor(1.0828, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2937, grad_fn=<SqrtBackward>) tensor(183.3969, grad_fn=<AddBackward0>) tensor(1.0812, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2919, grad_fn=<SqrtBackward>) tensor(183.1283, grad_fn=<AddBackward0>) tensor(1.0796, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2901, grad_fn=<SqrtBackward>) tensor(182.4480, grad_fn=<AddBackward0>) tensor(1.0780, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2883, grad_fn=<SqrtBackward>) tensor(181.8884, grad_fn=<AddBackward0>) tensor(1.0763, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2867, grad_fn=<SqrtBackward>) tensor(181.3079, grad_fn=<AddBackward0>) tensor(1.0746, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2851, grad_fn=<SqrtBackward>) tensor(180.8635, grad_fn=<AddBackward0>) tensor(1.0730, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2836, grad_fn=<SqrtBackward>) tensor(180.2144, grad_fn=<AddBackward0>) tensor(1.0713, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2821, grad_fn=<SqrtBackward>) tensor(179.5507, grad_fn=<AddBackward0>) tensor(1.0696, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2805, grad_fn=<SqrtBackward>) tensor(179.3109, grad_fn=<AddBackward0>) tensor(1.0679, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2788, grad_fn=<SqrtBackward>) tensor(178.4211, grad_fn=<AddBackward0>) tensor(1.0662, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2772, grad_fn=<SqrtBackward>) tensor(177.8956, grad_fn=<AddBackward0>) tensor(1.0646, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2754, grad_fn=<SqrtBackward>) tensor(177.4891, grad_fn=<AddBackward0>) tensor(1.0630, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2736, grad_fn=<SqrtBackward>) tensor(176.9036, grad_fn=<AddBackward0>) tensor(1.0615, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2719, grad_fn=<SqrtBackward>) tensor(176.4688, grad_fn=<AddBackward0>) tensor(1.0600, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2701, grad_fn=<SqrtBackward>) tensor(176.0014, grad_fn=<AddBackward0>) tensor(1.0586, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2684, grad_fn=<SqrtBackward>) tensor(175.3161, grad_fn=<AddBackward0>) tensor(1.0571, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2668, grad_fn=<SqrtBackward>) tensor(175.0086, grad_fn=<AddBackward0>) tensor(1.0557, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2653, grad_fn=<SqrtBackward>) tensor(174.3139, grad_fn=<AddBackward0>) tensor(1.0543, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2640, grad_fn=<SqrtBackward>) tensor(174.0079, grad_fn=<AddBackward0>) tensor(1.0529, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2627, grad_fn=<SqrtBackward>) tensor(173.5018, grad_fn=<AddBackward0>) tensor(1.0515, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2613, grad_fn=<SqrtBackward>) tensor(173.2831, grad_fn=<AddBackward0>) tensor(1.0501, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2600, grad_fn=<SqrtBackward>) tensor(172.6404, grad_fn=<AddBackward0>) tensor(1.0488, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2587, grad_fn=<SqrtBackward>) tensor(172.2934, grad_fn=<AddBackward0>) tensor(1.0475, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2574, grad_fn=<SqrtBackward>) tensor(171.9262, grad_fn=<AddBackward0>) tensor(1.0462, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2561, grad_fn=<SqrtBackward>) tensor(171.2974, grad_fn=<AddBackward0>) tensor(1.0449, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2549, grad_fn=<SqrtBackward>) tensor(170.9528, grad_fn=<AddBackward0>) tensor(1.0436, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2537, grad_fn=<SqrtBackward>) tensor(170.6688, grad_fn=<AddBackward0>) tensor(1.0423, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2525, grad_fn=<SqrtBackward>) tensor(170.2691, grad_fn=<AddBackward0>) tensor(1.0409, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2513, grad_fn=<SqrtBackward>) tensor(169.8710, grad_fn=<AddBackward0>) tensor(1.0396, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2502, grad_fn=<SqrtBackward>) tensor(169.3224, grad_fn=<AddBackward0>) tensor(1.0383, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2490, grad_fn=<SqrtBackward>) tensor(168.7258, grad_fn=<AddBackward0>) tensor(1.0370, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2478, grad_fn=<SqrtBackward>) tensor(168.4015, grad_fn=<AddBackward0>) tensor(1.0356, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2467, grad_fn=<SqrtBackward>) tensor(168.3400, grad_fn=<AddBackward0>) tensor(1.0343, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2454, grad_fn=<SqrtBackward>) tensor(167.4870, grad_fn=<AddBackward0>) tensor(1.0331, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2441, grad_fn=<SqrtBackward>) tensor(166.9906, grad_fn=<AddBackward0>) tensor(1.0318, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2427, grad_fn=<SqrtBackward>) tensor(166.7977, grad_fn=<AddBackward0>) tensor(1.0305, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2411, grad_fn=<SqrtBackward>) tensor(166.2751, grad_fn=<AddBackward0>) tensor(1.0293, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2396, grad_fn=<SqrtBackward>) tensor(165.9325, grad_fn=<AddBackward0>) tensor(1.0281, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2380, grad_fn=<SqrtBackward>) tensor(165.6951, grad_fn=<AddBackward0>) tensor(1.0268, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2364, grad_fn=<SqrtBackward>) tensor(165.0959, grad_fn=<AddBackward0>) tensor(1.0256, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2347, grad_fn=<SqrtBackward>) tensor(164.6328, grad_fn=<AddBackward0>) tensor(1.0244, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2331, grad_fn=<SqrtBackward>) tensor(164.2122, grad_fn=<AddBackward0>) tensor(1.0231, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2314, grad_fn=<SqrtBackward>) tensor(164.1227, grad_fn=<AddBackward0>) tensor(1.0219, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2299, grad_fn=<SqrtBackward>) tensor(163.3772, grad_fn=<AddBackward0>) tensor(1.0206, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2284, grad_fn=<SqrtBackward>) tensor(163.2903, grad_fn=<AddBackward0>) tensor(1.0193, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2267, grad_fn=<SqrtBackward>) tensor(162.9405, grad_fn=<AddBackward0>) tensor(1.0180, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2251, grad_fn=<SqrtBackward>) tensor(162.2830, grad_fn=<AddBackward0>) tensor(1.0168, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2235, grad_fn=<SqrtBackward>) tensor(161.9484, grad_fn=<AddBackward0>) tensor(1.0155, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2218, grad_fn=<SqrtBackward>) tensor(161.5819, grad_fn=<AddBackward0>) tensor(1.0142, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2202, grad_fn=<SqrtBackward>) tensor(161.2524, grad_fn=<AddBackward0>) tensor(1.0128, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2187, grad_fn=<SqrtBackward>) tensor(160.5392, grad_fn=<AddBackward0>) tensor(1.0114, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2173, grad_fn=<SqrtBackward>) tensor(160.3187, grad_fn=<AddBackward0>) tensor(1.0100, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2158, grad_fn=<SqrtBackward>) tensor(159.6520, grad_fn=<AddBackward0>) tensor(1.0086, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2143, grad_fn=<SqrtBackward>) tensor(159.2266, grad_fn=<AddBackward0>) tensor(1.0072, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2128, grad_fn=<SqrtBackward>) tensor(158.7556, grad_fn=<AddBackward0>) tensor(1.0057, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2113, grad_fn=<SqrtBackward>) tensor(158.4428, grad_fn=<AddBackward0>) tensor(1.0042, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2099, grad_fn=<SqrtBackward>) tensor(157.7991, grad_fn=<AddBackward0>) tensor(1.0027, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2085, grad_fn=<SqrtBackward>) tensor(157.4336, grad_fn=<AddBackward0>) tensor(1.0012, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2071, grad_fn=<SqrtBackward>) tensor(157.1474, grad_fn=<AddBackward0>) tensor(0.9998, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2058, grad_fn=<SqrtBackward>) tensor(156.4796, grad_fn=<AddBackward0>) tensor(0.9983, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2046, grad_fn=<SqrtBackward>) tensor(155.9966, grad_fn=<AddBackward0>) tensor(0.9969, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2034, grad_fn=<SqrtBackward>) tensor(155.9000, grad_fn=<AddBackward0>) tensor(0.9955, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2024, grad_fn=<SqrtBackward>) tensor(155.4498, grad_fn=<AddBackward0>) tensor(0.9941, grad_fn=<SqrtBackward>)\n",
      "tensor(1.2015, grad_fn=<SqrtBackward>) tensor(154.9458, grad_fn=<AddBackward0>) tensor(0.9927, grad_fn=<SqrtBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2005, grad_fn=<SqrtBackward>) tensor(154.3607, grad_fn=<AddBackward0>) tensor(0.9913, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1995, grad_fn=<SqrtBackward>) tensor(154.0081, grad_fn=<AddBackward0>) tensor(0.9900, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1984, grad_fn=<SqrtBackward>) tensor(153.5785, grad_fn=<AddBackward0>) tensor(0.9888, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1971, grad_fn=<SqrtBackward>) tensor(153.1530, grad_fn=<AddBackward0>) tensor(0.9876, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1957, grad_fn=<SqrtBackward>) tensor(152.7793, grad_fn=<AddBackward0>) tensor(0.9864, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1943, grad_fn=<SqrtBackward>) tensor(152.7220, grad_fn=<AddBackward0>) tensor(0.9853, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1930, grad_fn=<SqrtBackward>) tensor(152.1174, grad_fn=<AddBackward0>) tensor(0.9841, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1917, grad_fn=<SqrtBackward>) tensor(151.6809, grad_fn=<AddBackward0>) tensor(0.9829, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1905, grad_fn=<SqrtBackward>) tensor(151.3722, grad_fn=<AddBackward0>) tensor(0.9817, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1893, grad_fn=<SqrtBackward>) tensor(151.0079, grad_fn=<AddBackward0>) tensor(0.9805, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1880, grad_fn=<SqrtBackward>) tensor(150.6695, grad_fn=<AddBackward0>) tensor(0.9793, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1866, grad_fn=<SqrtBackward>) tensor(150.4300, grad_fn=<AddBackward0>) tensor(0.9782, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1850, grad_fn=<SqrtBackward>) tensor(149.8200, grad_fn=<AddBackward0>) tensor(0.9771, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1835, grad_fn=<SqrtBackward>) tensor(149.5196, grad_fn=<AddBackward0>) tensor(0.9759, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1820, grad_fn=<SqrtBackward>) tensor(149.1195, grad_fn=<AddBackward0>) tensor(0.9747, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1804, grad_fn=<SqrtBackward>) tensor(148.8805, grad_fn=<AddBackward0>) tensor(0.9735, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1789, grad_fn=<SqrtBackward>) tensor(148.6460, grad_fn=<AddBackward0>) tensor(0.9724, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1773, grad_fn=<SqrtBackward>) tensor(148.0900, grad_fn=<AddBackward0>) tensor(0.9713, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1758, grad_fn=<SqrtBackward>) tensor(147.9724, grad_fn=<AddBackward0>) tensor(0.9702, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1745, grad_fn=<SqrtBackward>) tensor(147.4566, grad_fn=<AddBackward0>) tensor(0.9691, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1732, grad_fn=<SqrtBackward>) tensor(147.1385, grad_fn=<AddBackward0>) tensor(0.9681, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1718, grad_fn=<SqrtBackward>) tensor(146.6656, grad_fn=<AddBackward0>) tensor(0.9670, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1703, grad_fn=<SqrtBackward>) tensor(146.4357, grad_fn=<AddBackward0>) tensor(0.9659, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1689, grad_fn=<SqrtBackward>) tensor(146.0284, grad_fn=<AddBackward0>) tensor(0.9649, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1675, grad_fn=<SqrtBackward>) tensor(145.7825, grad_fn=<AddBackward0>) tensor(0.9639, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1660, grad_fn=<SqrtBackward>) tensor(145.5698, grad_fn=<AddBackward0>) tensor(0.9629, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1647, grad_fn=<SqrtBackward>) tensor(145.2282, grad_fn=<AddBackward0>) tensor(0.9619, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1636, grad_fn=<SqrtBackward>) tensor(144.9632, grad_fn=<AddBackward0>) tensor(0.9611, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1624, grad_fn=<SqrtBackward>) tensor(144.9184, grad_fn=<AddBackward0>) tensor(0.9603, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1612, grad_fn=<SqrtBackward>) tensor(144.4005, grad_fn=<AddBackward0>) tensor(0.9595, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1602, grad_fn=<SqrtBackward>) tensor(144.1729, grad_fn=<AddBackward0>) tensor(0.9587, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1593, grad_fn=<SqrtBackward>) tensor(144.0952, grad_fn=<AddBackward0>) tensor(0.9580, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1584, grad_fn=<SqrtBackward>) tensor(143.8538, grad_fn=<AddBackward0>) tensor(0.9572, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1576, grad_fn=<SqrtBackward>) tensor(143.7032, grad_fn=<AddBackward0>) tensor(0.9565, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1569, grad_fn=<SqrtBackward>) tensor(143.3343, grad_fn=<AddBackward0>) tensor(0.9558, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1563, grad_fn=<SqrtBackward>) tensor(143.0581, grad_fn=<AddBackward0>) tensor(0.9550, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1557, grad_fn=<SqrtBackward>) tensor(142.8100, grad_fn=<AddBackward0>) tensor(0.9542, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1551, grad_fn=<SqrtBackward>) tensor(142.6460, grad_fn=<AddBackward0>) tensor(0.9534, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1542, grad_fn=<SqrtBackward>) tensor(142.5729, grad_fn=<AddBackward0>) tensor(0.9526, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1535, grad_fn=<SqrtBackward>) tensor(142.2904, grad_fn=<AddBackward0>) tensor(0.9518, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1527, grad_fn=<SqrtBackward>) tensor(142.0911, grad_fn=<AddBackward0>) tensor(0.9510, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1519, grad_fn=<SqrtBackward>) tensor(141.6749, grad_fn=<AddBackward0>) tensor(0.9502, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1512, grad_fn=<SqrtBackward>) tensor(141.5376, grad_fn=<AddBackward0>) tensor(0.9494, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1503, grad_fn=<SqrtBackward>) tensor(141.2953, grad_fn=<AddBackward0>) tensor(0.9486, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1496, grad_fn=<SqrtBackward>) tensor(141.0326, grad_fn=<AddBackward0>) tensor(0.9479, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1488, grad_fn=<SqrtBackward>) tensor(140.8844, grad_fn=<AddBackward0>) tensor(0.9471, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1480, grad_fn=<SqrtBackward>) tensor(140.6899, grad_fn=<AddBackward0>) tensor(0.9463, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1471, grad_fn=<SqrtBackward>) tensor(140.5818, grad_fn=<AddBackward0>) tensor(0.9456, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1460, grad_fn=<SqrtBackward>) tensor(140.3783, grad_fn=<AddBackward0>) tensor(0.9448, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1448, grad_fn=<SqrtBackward>) tensor(140.0320, grad_fn=<AddBackward0>) tensor(0.9441, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1437, grad_fn=<SqrtBackward>) tensor(139.8086, grad_fn=<AddBackward0>) tensor(0.9435, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1426, grad_fn=<SqrtBackward>) tensor(139.5737, grad_fn=<AddBackward0>) tensor(0.9428, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1417, grad_fn=<SqrtBackward>) tensor(139.3680, grad_fn=<AddBackward0>) tensor(0.9421, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1413, grad_fn=<SqrtBackward>) tensor(139.0817, grad_fn=<AddBackward0>) tensor(0.9414, grad_fn=<SqrtBackward>)\n",
      "tensor(1.1411, grad_fn=<SqrtBackward>) tensor(138.8090, grad_fn=<AddBackward0>) tensor(0.9408, grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "#training and testing\n",
    "n_users, n_items = 944, 1683\n",
    "loss_func = torch.nn.MSELoss()\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr= 1e-4, weight_decay = 1e-4 ) # learning rate\n",
    "\n",
    "l = np.array(list(range(79973)))\n",
    "\n",
    "for i in range(400):\n",
    "    random.shuffle(l)\n",
    "    loss_sum = 0\n",
    "    for ind in range(0, 79973, 512):\n",
    "            # Turn data into tensors\n",
    "            rating = Variable(torch.FloatTensor([train['rating'].iloc[l[ind: ind + 512]].values]).squeeze())\n",
    "            row = Variable(torch.LongTensor([train['user_id'].iloc[l[ind: ind + 512]].values]).squeeze())\n",
    "            col = Variable(torch.LongTensor([train['item_id'].iloc[l[ind: ind + 512]].values]).squeeze())\n",
    "\n",
    "            # Predict and calculate loss\n",
    "            prediction = model(row, col)\n",
    "            loss = loss_func(prediction, rating)\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "    print(evaluator(model, test), loss_sum, evaluator(model, train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. plotting using plotly\n",
    "2. try different optimizer and see the result\n",
    "3. wanshan review optimizer after work\n",
    "4. Why use variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take away\n",
    "initialization is very important\n",
    "train on sparse stuff is very slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoRec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def: identify CF with an autoencoder architecture and aims to integrate nonlinear transformations into \n",
    "    CF on the basis of explicit feedback\n",
    "structure: an input layer, a hidden layer, and a reconstruction layer. \n",
    "    use column/row of the interaction matrix as an input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:43:30.715401Z",
     "start_time": "2020-07-21T13:43:30.683956Z"
    }
   },
   "outputs": [],
   "source": [
    "class AutoRec(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users,  n_factors=20, dropout = 0.05):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Linear(n_users, n_factors,  bias=True)\n",
    "        self.act = torch.nn.Sigmoid()\n",
    "        self.decoder = torch.nn.Linear(n_factors, n_users, bias=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        #self.relu = torch.nn.ReLU(inplace=False)\n",
    "        torch.nn.init.normal_(self.encoder.weight, 0.01)\n",
    "        torch.nn.init.normal_(self.decoder.weight, 0.01)\n",
    "        #torch.nn.init.normal_(self.act.weight, 0.01)\n",
    "        \n",
    "    def forward(self, user, is_train=1 ): #squeeze remove dimension equals to 1\n",
    "        hidden = self.dropout(self.act(self.encoder(user)))\n",
    "        #print(hidden)\n",
    "        pred = self.decoder(hidden)\n",
    "#         if autograd.is_training():  # Mask the gradient during training\n",
    "#             return pred * np.sign(input) \n",
    "#         else:\n",
    "#             \n",
    "        if is_train == 1:\n",
    "            pred = pred * np.sign(user)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:43:32.819723Z",
     "start_time": "2020-07-21T13:43:32.817053Z"
    }
   },
   "outputs": [],
   "source": [
    "n_users = 943\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:43:33.138018Z",
     "start_time": "2020-07-21T13:43:33.122856Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'pivot_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-399-774aab626463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'pivot_table'"
     ]
    }
   ],
   "source": [
    "train_trans = train.pivot_table(index='item_id', columns='user_id').loc[:, 'rating']\n",
    "train_trans = train_trans.reset_index().drop('item_id', axis = 1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:43:33.354234Z",
     "start_time": "2020-07-21T13:43:33.336317Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'pivot_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-400-9fd0d92fb162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m944\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtest_trans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'pivot_table'"
     ]
    }
   ],
   "source": [
    "test_trans = test.pivot_table(index='item_id', columns='user_id').loc[:, 'rating']\n",
    "test_trans = test_trans.reset_index().drop('item_id', axis = 1).fillna(0)\n",
    "for i in range(1, 944):\n",
    "    if i not in test_trans.columns:\n",
    "        test_trans[i] = 0\n",
    "test_trans = test_trans.sort_index(ascending=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:43:42.703491Z",
     "start_time": "2020-07-21T13:43:42.697543Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_matrix(test_trans):\n",
    "    val = (model(Variable(torch.FloatTensor(test_trans.values))).detach().numpy() - test_trans.values)**2\n",
    "    #print(val)\n",
    "    sign = np.sign(test_trans.values)\n",
    "\n",
    "    return np.sqrt(np.sum(val * sign)/np.sum(sign))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于 loss\n",
    "\n",
    "loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:\n",
    "\n",
    "x.grad += dloss/dx\n",
    "optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs:\n",
    "\n",
    "x += -lr * x.grad\n",
    "optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n",
    "\n",
    "If you have multiple losses (loss1, loss2) you can sum them and then call backwards once:\n",
    "\n",
    "loss3 = loss1 + loss2\n",
    "loss3.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:26:49.432312Z",
     "start_time": "2020-07-21T14:26:49.427380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4953,  1.5546,  1.4921,  0.4234])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:24:58.228188Z",
     "start_time": "2020-07-21T14:24:58.224270Z"
    }
   },
   "source": [
    "对于 istraining 比 在拿到pred 用sign 更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:56:49.568639Z",
     "start_time": "2020-07-21T13:51:07.754497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.683326210634394 tensor(88.4654, grad_fn=<AddBackward0>) 14.797309719910269\n",
      "12.475249047027127 tensor(42.4289, grad_fn=<AddBackward0>) 11.199343685069225\n",
      "12.276483609264808 tensor(33.4972, grad_fn=<AddBackward0>) 9.896485949835231\n",
      "11.022524319563313 tensor(26.4467, grad_fn=<AddBackward0>) 8.9694888470072\n",
      "9.484044306429725 tensor(21.3801, grad_fn=<AddBackward0>) 7.95752944927133\n",
      "7.964184591698304 tensor(15.8457, grad_fn=<AddBackward0>) 6.886913289981216\n",
      "6.843455180860399 tensor(11.9608, grad_fn=<AddBackward0>) 5.950099806256724\n",
      "5.90475264609777 tensor(8.2655, grad_fn=<AddBackward0>) 4.985017883117118\n",
      "5.147927477276663 tensor(6.0361, grad_fn=<AddBackward0>) 4.294854373047684\n",
      "4.546448059622635 tensor(4.8081, grad_fn=<AddBackward0>) 3.8006649904748198\n",
      "4.0556940215884945 tensor(3.7723, grad_fn=<AddBackward0>) 3.3880557759090526\n",
      "3.641942445492659 tensor(3.1096, grad_fn=<AddBackward0>) 3.0642911719193533\n",
      "3.3031199671144416 tensor(2.5150, grad_fn=<AddBackward0>) 2.7761436186668624\n",
      "3.0189609395938124 tensor(2.0955, grad_fn=<AddBackward0>) 2.536655715304314\n",
      "2.771066959438655 tensor(1.8066, grad_fn=<AddBackward0>) 2.3390508184724164\n",
      "2.572145617036963 tensor(1.5721, grad_fn=<AddBackward0>) 2.1930920389079365\n",
      "2.410551102895394 tensor(1.4754, grad_fn=<AddBackward0>) 2.101150084610094\n",
      "2.2946238418713523 tensor(1.3943, grad_fn=<AddBackward0>) 2.055876879503499\n",
      "2.215802969735319 tensor(1.3391, grad_fn=<AddBackward0>) 2.0076372835744496\n",
      "2.167462342895875 tensor(1.2842, grad_fn=<AddBackward0>) 1.9516557315626355\n",
      "2.1404472969946964 tensor(1.2929, grad_fn=<AddBackward0>) 1.967438859082061\n",
      "2.153221126055412 tensor(1.3897, grad_fn=<AddBackward0>) 2.028110441286762\n",
      "2.1789268210186488 tensor(1.4719, grad_fn=<AddBackward0>) 2.0911355828500113\n",
      "2.197816799358841 tensor(1.5635, grad_fn=<AddBackward0>) 2.1389794488870906\n",
      "2.198970288729983 tensor(1.6126, grad_fn=<AddBackward0>) 2.156601156920674\n",
      "2.1757496533662484 tensor(1.5539, grad_fn=<AddBackward0>) 2.1405915116034073\n",
      "2.138508610381834 tensor(1.5076, grad_fn=<AddBackward0>) 2.0981698604207093\n",
      "2.0800196376314926 tensor(1.4278, grad_fn=<AddBackward0>) 2.038147884928311\n",
      "2.014752802526856 tensor(1.3213, grad_fn=<AddBackward0>) 1.9701091805950908\n",
      "1.9511535692784492 tensor(1.2364, grad_fn=<AddBackward0>) 1.901231422130894\n",
      "1.9097884645532845 tensor(1.1448, grad_fn=<AddBackward0>) 1.8309961698217212\n",
      "1.8906548364456066 tensor(1.0840, grad_fn=<AddBackward0>) 1.7885411173060903\n",
      "1.8913786402755999 tensor(1.0563, grad_fn=<AddBackward0>) 1.7701171134259666\n",
      "1.9066041910334697 tensor(1.0862, grad_fn=<AddBackward0>) 1.7840179507701106\n",
      "1.9360208972722108 tensor(1.0929, grad_fn=<AddBackward0>) 1.8044550697405903\n",
      "1.957463781849139 tensor(1.0999, grad_fn=<AddBackward0>) 1.818970674536119\n",
      "1.9755081534670722 tensor(1.1368, grad_fn=<AddBackward0>) 1.8387587641938807\n",
      "1.9915148640174878 tensor(1.1566, grad_fn=<AddBackward0>) 1.8575147379433765\n",
      "1.9989166400298568 tensor(1.1539, grad_fn=<AddBackward0>) 1.8677103642625432\n",
      "1.9954819987442622 tensor(1.1614, grad_fn=<AddBackward0>) 1.8634860691711714\n",
      "1.9746757615552641 tensor(1.1351, grad_fn=<AddBackward0>) 1.8400303074297486\n",
      "1.9433508421546886 tensor(1.0609, grad_fn=<AddBackward0>) 1.7950482273069444\n",
      "1.9120038646850395 tensor(1.0278, grad_fn=<AddBackward0>) 1.7544154992683882\n",
      "1.883795021583302 tensor(0.9763, grad_fn=<AddBackward0>) 1.71859259234879\n",
      "1.862630234090203 tensor(0.9363, grad_fn=<AddBackward0>) 1.6870271727008805\n",
      "1.8450689888643566 tensor(0.9078, grad_fn=<AddBackward0>) 1.6595199531501894\n",
      "1.838153891284403 tensor(0.8700, grad_fn=<AddBackward0>) 1.638717330994821\n",
      "1.8432227333827234 tensor(0.8528, grad_fn=<AddBackward0>) 1.6264863515615788\n",
      "1.8605072003929524 tensor(0.8498, grad_fn=<AddBackward0>) 1.621635279193616\n",
      "1.8915533600866816 tensor(0.8512, grad_fn=<AddBackward0>) 1.623637945595776\n",
      "1.926666936558808 tensor(0.8547, grad_fn=<AddBackward0>) 1.6331975163615866\n",
      "1.965972453916774 tensor(0.8591, grad_fn=<AddBackward0>) 1.642340312946548\n",
      "2.011706422074058 tensor(0.8787, grad_fn=<AddBackward0>) 1.6557875092641612\n",
      "2.0595558229129094 tensor(0.8858, grad_fn=<AddBackward0>) 1.6701668815126016\n",
      "2.1195279799729168 tensor(0.8872, grad_fn=<AddBackward0>) 1.6835590229336455\n",
      "2.195125361641015 tensor(0.9056, grad_fn=<AddBackward0>) 1.7006320033817655\n",
      "2.285292078590041 tensor(0.9188, grad_fn=<AddBackward0>) 1.7160273166683355\n",
      "2.375344852714667 tensor(0.9373, grad_fn=<AddBackward0>) 1.7267998848593382\n",
      "2.468042312804954 tensor(0.9360, grad_fn=<AddBackward0>) 1.7368102459570014\n",
      "2.5823725608462005 tensor(0.9319, grad_fn=<AddBackward0>) 1.7394434734426196\n",
      "2.6924132070514775 tensor(0.9216, grad_fn=<AddBackward0>) 1.7349328979259346\n",
      "2.8127896475833896 tensor(0.8769, grad_fn=<AddBackward0>) 1.7323588250175173\n",
      "2.954262692986089 tensor(0.8592, grad_fn=<AddBackward0>) 1.7329802200255846\n",
      "3.1083122440218904 tensor(0.8601, grad_fn=<AddBackward0>) 1.7485828204539922\n",
      "3.2648627665306447 tensor(0.8500, grad_fn=<AddBackward0>) 1.7639919456844693\n",
      "3.4339478521013445 tensor(0.8478, grad_fn=<AddBackward0>) 1.7874437589009562\n",
      "3.627441681883199 tensor(0.8355, grad_fn=<AddBackward0>) 1.8079639197599637\n",
      "3.8315927632639672 tensor(0.8236, grad_fn=<AddBackward0>) 1.8329295309024856\n",
      "4.056370027804197 tensor(0.8275, grad_fn=<AddBackward0>) 1.8620470263639772\n",
      "4.279150700522013 tensor(0.8228, grad_fn=<AddBackward0>) 1.8911119911301424\n",
      "4.522170422509218 tensor(0.8233, grad_fn=<AddBackward0>) 1.9222060067290985\n",
      "4.803023715654088 tensor(0.8302, grad_fn=<AddBackward0>) 1.9482372021877616\n",
      "5.076761506501229 tensor(0.8096, grad_fn=<AddBackward0>) 1.9806449056974218\n",
      "5.346705168419696 tensor(0.7942, grad_fn=<AddBackward0>) 2.0284738178017028\n",
      "5.625681889431246 tensor(0.8009, grad_fn=<AddBackward0>) 2.071098018063203\n",
      "5.959981909898929 tensor(0.8047, grad_fn=<AddBackward0>) 2.1187671642736934\n",
      "6.342557188611643 tensor(0.7974, grad_fn=<AddBackward0>) 2.165389641892912\n",
      "6.689457947289383 tensor(0.8010, grad_fn=<AddBackward0>) 2.220378885185384\n",
      "7.033068314801521 tensor(0.7995, grad_fn=<AddBackward0>) 2.2837311596758227\n",
      "7.367720606986986 tensor(0.8115, grad_fn=<AddBackward0>) 2.3461125037618373\n",
      "7.694568797965117 tensor(0.8113, grad_fn=<AddBackward0>) 2.4097322319076144\n",
      "8.000826908047763 tensor(0.8071, grad_fn=<AddBackward0>) 2.474323613934532\n",
      "8.30807805015426 tensor(0.8047, grad_fn=<AddBackward0>) 2.5377309360284053\n",
      "8.606341601482706 tensor(0.8239, grad_fn=<AddBackward0>) 2.5983106385649006\n",
      "8.985793092758358 tensor(0.8275, grad_fn=<AddBackward0>) 2.663949526020697\n",
      "9.324685603064044 tensor(0.8507, grad_fn=<AddBackward0>) 2.7160638812582945\n",
      "9.611224517995018 tensor(0.8642, grad_fn=<AddBackward0>) 2.7548668026065273\n",
      "9.900062423379126 tensor(0.8821, grad_fn=<AddBackward0>) 2.7936786849382416\n",
      "10.191626341616175 tensor(0.8515, grad_fn=<AddBackward0>) 2.806103890677676\n",
      "10.448703152853575 tensor(0.8546, grad_fn=<AddBackward0>) 2.8383598135447694\n",
      "10.763842465467878 tensor(0.8469, grad_fn=<AddBackward0>) 2.8724435527396173\n",
      "11.038026003038912 tensor(0.8548, grad_fn=<AddBackward0>) 2.916918192969761\n",
      "11.303368607666638 tensor(0.8950, grad_fn=<AddBackward0>) 2.9736689444557616\n",
      "11.615463085815486 tensor(0.9239, grad_fn=<AddBackward0>) 3.0070521021174987\n",
      "11.89348117586525 tensor(0.9057, grad_fn=<AddBackward0>) 3.031764063512938\n",
      "12.09145529134289 tensor(0.9045, grad_fn=<AddBackward0>) 3.0671133214292947\n",
      "12.215159186674507 tensor(0.9055, grad_fn=<AddBackward0>) 3.0966574003092937\n",
      "12.4209093198636 tensor(0.9214, grad_fn=<AddBackward0>) 3.127638698817865\n",
      "12.619461365367963 tensor(0.9398, grad_fn=<AddBackward0>) 3.154267666387915\n",
      "12.743049493875061 tensor(0.9554, grad_fn=<AddBackward0>) 3.2042672470111833\n"
     ]
    }
   ],
   "source": [
    "n_users, n_items = 943, 1683\n",
    "loss_func = torch.nn.MSELoss()\n",
    "model = AutoRec(n_users, n_factors=100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr= 0.002, weight_decay = 1e-5 ) # learning rate\n",
    "\n",
    "l = np.array(list(range(943)))\n",
    "\n",
    "test, train = [], []\n",
    "for i in range(1000):\n",
    "    random.shuffle(l)\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    for ind in range(0, 943, 256):\n",
    "            # Turn data into tensors\n",
    "            rating = Variable(torch.FloatTensor([train_trans.iloc[l[ind: ind + 256]].values]))\n",
    "            \n",
    "            # Predict and calculate loss\n",
    "            prediction = model(rating)\n",
    "            #method1: register and match\n",
    "            #prediction.register_hook(lambda grad: grad * torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values) ))\n",
    "            \n",
    "            #method 1 mask after model\n",
    "            sign = torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values))\n",
    "            loss = loss_func(prediction * sign, rating * sign)\n",
    "\n",
    "            #mask in the model\n",
    "            loss = loss_func(prediction , rating )\n",
    "            \n",
    "                \n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        model.eval()\n",
    "        test.append(evaluate_matrix(test_trans))\n",
    "        train.append(evaluate_matrix(train_trans))\n",
    "        print(evaluate_matrix(test_trans), loss_sum, evaluate_matrix(train_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T13:57:30.026739Z",
     "start_time": "2020-07-21T13:57:30.020334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.683326210634394,\n",
       " 12.475249047027127,\n",
       " 12.276483609264808,\n",
       " 11.022524319563313,\n",
       " 9.484044306429725,\n",
       " 7.964184591698304,\n",
       " 6.843455180860399,\n",
       " 5.90475264609777,\n",
       " 5.147927477276663,\n",
       " 4.546448059622635,\n",
       " 4.0556940215884945,\n",
       " 3.641942445492659,\n",
       " 3.3031199671144416,\n",
       " 3.0189609395938124,\n",
       " 2.771066959438655,\n",
       " 2.572145617036963,\n",
       " 2.410551102895394,\n",
       " 2.2946238418713523,\n",
       " 2.215802969735319,\n",
       " 2.167462342895875,\n",
       " 2.1404472969946964,\n",
       " 2.153221126055412,\n",
       " 2.1789268210186488,\n",
       " 2.197816799358841,\n",
       " 2.198970288729983,\n",
       " 2.1757496533662484,\n",
       " 2.138508610381834,\n",
       " 2.0800196376314926,\n",
       " 2.014752802526856,\n",
       " 1.9511535692784492,\n",
       " 1.9097884645532845,\n",
       " 1.8906548364456066,\n",
       " 1.8913786402755999,\n",
       " 1.9066041910334697,\n",
       " 1.9360208972722108,\n",
       " 1.957463781849139,\n",
       " 1.9755081534670722,\n",
       " 1.9915148640174878,\n",
       " 1.9989166400298568,\n",
       " 1.9954819987442622,\n",
       " 1.9746757615552641,\n",
       " 1.9433508421546886,\n",
       " 1.9120038646850395,\n",
       " 1.883795021583302,\n",
       " 1.862630234090203,\n",
       " 1.8450689888643566,\n",
       " 1.838153891284403,\n",
       " 1.8432227333827234,\n",
       " 1.8605072003929524,\n",
       " 1.8915533600866816,\n",
       " 1.926666936558808,\n",
       " 1.965972453916774,\n",
       " 2.011706422074058,\n",
       " 2.0595558229129094,\n",
       " 2.1195279799729168,\n",
       " 2.195125361641015,\n",
       " 2.285292078590041,\n",
       " 2.375344852714667,\n",
       " 2.468042312804954,\n",
       " 2.5823725608462005,\n",
       " 2.6924132070514775,\n",
       " 2.8127896475833896,\n",
       " 2.954262692986089,\n",
       " 3.1083122440218904,\n",
       " 3.2648627665306447,\n",
       " 3.4339478521013445,\n",
       " 3.627441681883199,\n",
       " 3.8315927632639672,\n",
       " 4.056370027804197,\n",
       " 4.279150700522013,\n",
       " 4.522170422509218,\n",
       " 4.803023715654088,\n",
       " 5.076761506501229,\n",
       " 5.346705168419696,\n",
       " 5.625681889431246,\n",
       " 5.959981909898929,\n",
       " 6.342557188611643,\n",
       " 6.689457947289383,\n",
       " 7.033068314801521,\n",
       " 7.367720606986986,\n",
       " 7.694568797965117,\n",
       " 8.000826908047763,\n",
       " 8.30807805015426,\n",
       " 8.606341601482706,\n",
       " 8.985793092758358,\n",
       " 9.324685603064044,\n",
       " 9.611224517995018,\n",
       " 9.900062423379126,\n",
       " 10.191626341616175,\n",
       " 10.448703152853575,\n",
       " 10.763842465467878,\n",
       " 11.038026003038912,\n",
       " 11.303368607666638,\n",
       " 11.615463085815486,\n",
       " 11.89348117586525,\n",
       " 12.09145529134289,\n",
       " 12.215159186674507,\n",
       " 12.4209093198636,\n",
       " 12.619461365367963,\n",
       " 12.743049493875061]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:44:44.209435Z",
     "start_time": "2020-07-16T14:44:16.182418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly==4.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/5f/47ab0d9d843c5be0f5c5bd891736a4c84fa45c3b0a0ddb6b6df7c098c66f/plotly-4.9.0-py2.py3-none-any.whl (12.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/two/anaconda3/lib/python3.6/site-packages (from plotly==4.9.0) (1.11.0)\n",
      "Collecting retrying>=1.3.3 (from plotly==4.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/ef/beae4b4ef80902f22e3af073397f079c96969c69b2c7d52a57ea9ae61c9d/retrying-1.3.3.tar.gz\n",
      "Building wheels for collected packages: retrying\n",
      "  Running setup.py bdist_wheel for retrying ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/two/Library/Caches/pip/wheels/d7/a9/33/acc7b709e2a35caa7d4cae442f6fe6fbf2c43f80823d46460c\n",
      "Successfully built retrying\n",
      "Installing collected packages: retrying, plotly\n",
      "Successfully installed plotly-4.9.0 retrying-1.3.3\n",
      "\u001b[33mYou are using pip version 18.0, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly==4.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T14:14:31.568331Z",
     "start_time": "2020-07-17T14:14:31.561620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.936595359627366,\n",
       " 4.308426273341203,\n",
       " 3.400383175321424,\n",
       " 2.895760221532256,\n",
       " 2.556590823070144,\n",
       " 2.3519294807880846,\n",
       " 2.1920607502285985,\n",
       " 2.066014936505195,\n",
       " 1.999229770205389,\n",
       " 1.9863220345946528,\n",
       " 2.001368078433109,\n",
       " 2.0221929218828563,\n",
       " 2.0450972254915074,\n",
       " 2.061240292249084,\n",
       " 2.0547907136820602,\n",
       " 2.017839706163737,\n",
       " 1.9668001342790007,\n",
       " 1.9042236409901392,\n",
       " 1.852773555001788,\n",
       " 1.81562902566686,\n",
       " 1.7862917973230548,\n",
       " 1.7617957910911841,\n",
       " 1.7477417344970652,\n",
       " 1.7444745455949855,\n",
       " 1.7495308755006713,\n",
       " 1.7555877941882585,\n",
       " 1.7489026038917135,\n",
       " 1.7304830668159465,\n",
       " 1.7046637000176936,\n",
       " 1.6815836925583632,\n",
       " 1.6697358620909744,\n",
       " 1.66673647207834,\n",
       " 1.6711707790867665,\n",
       " 1.6810399410943038,\n",
       " 1.6953374153046732,\n",
       " 1.7049213665580103,\n",
       " 1.706897358212583,\n",
       " 1.6970051182339807,\n",
       " 1.6808250745423,\n",
       " 1.6684437423639573,\n",
       " 1.6712576263923065,\n",
       " 1.6830173870831635,\n",
       " 1.7006157275330536,\n",
       " 1.7118038798303483,\n",
       " 1.717989274832314,\n",
       " 1.7310250241542544,\n",
       " 1.746893515545705,\n",
       " 1.7671204530697173,\n",
       " 1.7945164460499754,\n",
       " 1.8250562812350164,\n",
       " 1.8580514735963058,\n",
       " 1.9106050281034348,\n",
       " 1.9652180639438595,\n",
       " 2.0371757917123867,\n",
       " 2.1107813419080843,\n",
       " 2.1756082573494813,\n",
       " 2.239126693698647,\n",
       " 2.306960361631763,\n",
       " 2.4077757572072036,\n",
       " 2.502901916903956,\n",
       " 2.618752371409355,\n",
       " 2.7514970924151108,\n",
       " 2.8817061339231964,\n",
       " 3.029280417736983,\n",
       " 3.144334831164781,\n",
       " 3.2855601159027104,\n",
       " 3.4266498056234527,\n",
       " 3.5873160469138816,\n",
       " 3.7389318918851555,\n",
       " 3.8774669255282403,\n",
       " 3.940890267913589,\n",
       " 3.9955604719398052,\n",
       " 4.009795968040074,\n",
       " 4.0468898502899675,\n",
       " 4.053847701927714,\n",
       " 4.06132683608035,\n",
       " 4.128933480589566,\n",
       " 4.190075220361675,\n",
       " 4.234014276481345,\n",
       " 4.245506003979978,\n",
       " 4.205186784312935,\n",
       " 4.1500801832224825,\n",
       " 4.141937907676228,\n",
       " 4.122121190763147,\n",
       " 4.156391070736247,\n",
       " 4.153793228394223,\n",
       " 4.110132255127044,\n",
       " 4.04062602407652,\n",
       " 3.974545727499247,\n",
       " 3.874991398328465,\n",
       " 3.8047200119445073,\n",
       " 3.790585525499402,\n",
       " 3.764895018544256,\n",
       " 3.7059651608867705,\n",
       " 3.6998227297299238,\n",
       " 3.6959444056156463,\n",
       " 3.68176299664971,\n",
       " 3.6938838499510043,\n",
       " 3.7132921060245794,\n",
       " 3.706530703402625]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:35:37.997607Z",
     "start_time": "2020-07-22T14:35:37.991359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.        ,   199.53535354,   399.07070707,   598.60606061,\n",
       "         798.14141414,   997.67676768,  1197.21212121,  1396.74747475,\n",
       "        1596.28282828,  1795.81818182,  1995.35353535,  2194.88888889,\n",
       "        2394.42424242,  2593.95959596,  2793.49494949,  2993.03030303,\n",
       "        3192.56565657,  3392.1010101 ,  3591.63636364,  3791.17171717,\n",
       "        3990.70707071,  4190.24242424,  4389.77777778,  4589.31313131,\n",
       "        4788.84848485,  4988.38383838,  5187.91919192,  5387.45454545,\n",
       "        5586.98989899,  5786.52525253,  5986.06060606,  6185.5959596 ,\n",
       "        6385.13131313,  6584.66666667,  6784.2020202 ,  6983.73737374,\n",
       "        7183.27272727,  7382.80808081,  7582.34343434,  7781.87878788,\n",
       "        7981.41414141,  8180.94949495,  8380.48484848,  8580.02020202,\n",
       "        8779.55555556,  8979.09090909,  9178.62626263,  9378.16161616,\n",
       "        9577.6969697 ,  9777.23232323,  9976.76767677, 10176.3030303 ,\n",
       "       10375.83838384, 10575.37373737, 10774.90909091, 10974.44444444,\n",
       "       11173.97979798, 11373.51515152, 11573.05050505, 11772.58585859,\n",
       "       11972.12121212, 12171.65656566, 12371.19191919, 12570.72727273,\n",
       "       12770.26262626, 12969.7979798 , 13169.33333333, 13368.86868687,\n",
       "       13568.4040404 , 13767.93939394, 13967.47474747, 14167.01010101,\n",
       "       14366.54545455, 14566.08080808, 14765.61616162, 14965.15151515,\n",
       "       15164.68686869, 15364.22222222, 15563.75757576, 15763.29292929,\n",
       "       15962.82828283, 16162.36363636, 16361.8989899 , 16561.43434343,\n",
       "       16760.96969697, 16960.50505051, 17160.04040404, 17359.57575758,\n",
       "       17559.11111111, 17758.64646465, 17958.18181818, 18157.71717172,\n",
       "       18357.25252525, 18556.78787879, 18756.32323232, 18955.85858586,\n",
       "       19155.39393939, 19354.92929293, 19554.46464646, 19754.        ])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:35:32.110106Z",
     "start_time": "2020-07-22T14:35:31.932065Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-436-0bcd53cbcef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# labeling the right_side of the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m annotations.append(dict(xref='paper', x=1.07, y=test[-1],\n\u001b[0m\u001b[1;32m     56\u001b[0m                               \u001b[0mxanchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                               \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "N = 100\n",
    "random_x = np.linspace(0, len(test), 100)\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=random_x, y=test,\n",
    "                    mode='lines',\n",
    "                    name='test accuracy', \n",
    "                        marker_color='rgb(37,37,37)',\n",
    "                        connectgaps=True))\n",
    "fig.add_trace(go.Scatter(x=random_x, y=train,\n",
    "                    mode='lines',\n",
    "                    name='train accuracy'\n",
    "                        ,marker_color='rgb(150,150,150)',\n",
    "                        connectgaps=True))\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        showline=False,\n",
    "        showgrid=False,\n",
    "        showticklabels=False,\n",
    "#         linecolor='rgb(204, 204, 204)',\n",
    "#         linewidth=2,\n",
    "#         ticks='outside',\n",
    "#         tickfont=dict(\n",
    "#             family='Arial',\n",
    "#             size=12,\n",
    "#             color='rgb(82, 82, 82)',\n",
    "#        ),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        showticklabels=True,\n",
    "    ),\n",
    "    autosize=False,\n",
    "    margin=dict(\n",
    "        autoexpand=False,\n",
    "        l=120,\n",
    "        r=40,\n",
    "        t=130,\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "annotations = []\n",
    "# labeling the right_side of the plot\n",
    "annotations.append(dict(xref='paper', x=1.07, y=test[-1],\n",
    "                              xanchor='right',\n",
    "                              text='test',\n",
    "                              font=dict(family='Arial',\n",
    "                                        size=16,\n",
    "                                        color='rgb(37,37,37)'),\n",
    "                              showarrow=False\n",
    "                              \n",
    "                             ))\n",
    "# Source\n",
    "\n",
    "annotations.append(dict(xref='paper', x=1.07, y=train[-1],\n",
    "                              xanchor='right',\n",
    "                              text='train',\n",
    "                              font=dict(family='Arial',\n",
    "                                        size=16,\n",
    "                                        color='rgb(150,150,150)'),\n",
    "                              showarrow=False\n",
    "                              \n",
    "                             ))\n",
    "# annotations.append(dict(xref='paper', yref='paper', x=0.5, y=-0.1,\n",
    "#                               xanchor='center', yanchor='top',\n",
    "#                               text='Source: PewResearch Center & ' +\n",
    "#                                    'Storytelling with data',\n",
    "#                               font=dict(family='Arial',\n",
    "#                                         size=12,\n",
    "#                                         color='rgb(150,150,150)'),\n",
    "#                               showarrow=False))\n",
    "\n",
    "# Title\n",
    "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text='AutoRec Accuracy',\n",
    "                              font=dict(family='Arial',\n",
    "                                        size=30,\n",
    "                                        color='rgb(150,150,150)'),\n",
    "                              showarrow=False))\n",
    "\n",
    "fig.update_layout(annotations=annotations)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Ranking for Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem of explicit feedback: in real world, most feedback is implicit; non-observed yser-item pairs which may be predictive for users' interests are totally ignored (bad for missing not at random).\n",
    "generated ranked recommendation lists from implicit feedback have gained popularity:\n",
    "    pointwise: considers a single interaction at a time\n",
    "    pairwise: a pair of items for each user and aim to approximate the optimal ordering for that pairs\n",
    "    listwise: approximate the ordering of the entire list of items(NDCG), complex and compute intensive\n",
    "\n",
    "Bayesian Personalized Ranking Loss\n",
    "assumption: user prefers the positive item over all other non-observed items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "    1 clean data to see which one we have, which one we do not\n",
    "    2 compare users and see the result\n",
    "    3 return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:36:03.630114Z",
     "start_time": "2020-07-22T14:36:03.595944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>934</th>\n",
       "      <th>935</th>\n",
       "      <th>936</th>\n",
       "      <th>937</th>\n",
       "      <th>938</th>\n",
       "      <th>939</th>\n",
       "      <th>940</th>\n",
       "      <th>941</th>\n",
       "      <th>942</th>\n",
       "      <th>943</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1657 rows × 943 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id  1    2    3    4    5    6    7    8    9    10   ...  934  935  936  \\\n",
       "0        5.0  4.0  0.0  0.0  4.0  0.0  0.0  0.0  0.0  4.0  ...  2.0  3.0  4.0   \n",
       "1        0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  0.0  ...  4.0  0.0  0.0   \n",
       "2        4.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  4.0   \n",
       "3        3.0  0.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  ...  5.0  0.0  0.0   \n",
       "4        3.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1652     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1653     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1654     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1655     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1656     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "user_id  937  938  939  940  941  942  943  \n",
       "0        0.0  4.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1        0.0  0.0  0.0  0.0  0.0  0.0  5.0  \n",
       "2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  \n",
       "1652     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1653     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1654     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1655     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1656     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1657 rows x 943 columns]"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T13:22:42.043104Z",
     "start_time": "2020-07-23T13:22:42.037289Z"
    }
   },
   "outputs": [],
   "source": [
    "embeds = torch.nn.Embedding(2, 1) #第一个是dim 第二个是dim大小\n",
    "a = torch.matmul(embeds(torch.tensor([1])), embeds(torch.tensor([1,0])).T )\n",
    "b = torch.matmul(embeds(torch.tensor([1])), embeds(torch.tensor([1,0])).T )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T13:36:54.338162Z",
     "start_time": "2020-07-23T13:36:54.321175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6182)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:24:56.489159Z",
     "start_time": "2020-07-23T14:24:56.448419Z"
    }
   },
   "outputs": [],
   "source": [
    "#在里面做loss\n",
    "#embedding (user, factor)\n",
    "#multiply 然后写loss function + regularize\n",
    "\n",
    "class BPRloss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items,  n_factors=2):\n",
    "        super().__init__()\n",
    "        self.user_matrix = torch.nn.Embedding(n_users, n_factors)\n",
    "        self.item_matrix = torch.nn.Embedding(n_items, n_factors)\n",
    "        torch.nn.init.normal_(self.user_matrix.weight, 0.001)\n",
    "        torch.nn.init.normal_(self.item_matrix.weight, 0.001)\n",
    "        #torch.nn.init.normal_(self.act.weight, 0.01)\n",
    "        self.m = torch.nn.Sigmoid()\n",
    "    def forward(self, user, positive, negative): #squeeze remove dimension equals to 1\n",
    "        #print(user)\n",
    "        #print(self.user_matrix(user))\n",
    "        um = torch.matmul(self.user_matrix(user), self.item_matrix(positive).T ).reshape(-1) \n",
    "        im = torch.matmul(self.user_matrix(user), self.item_matrix(negative).T ).reshape(-1) \n",
    "        \n",
    "        #print(im)\n",
    "        um_shape = um.shape[0]\n",
    "        im_shape = im.shape[0]\n",
    "        um_m = um.unsqueeze_(-1).expand(um_shape,im_shape)\n",
    "        im_m = im.unsqueeze_(-1).expand(im_shape,um_shape).T\n",
    "        #print(im_m)\n",
    "        #print((um_m  - im_m).reshape(-1))\n",
    "        return - torch.log(self.m((um_m  - im_m).reshape(-1))).sum()\n",
    "        #         if autograd.is_training():  # Mask the gradient during training\n",
    "#             return pred * np.sign(input) \n",
    "#         else:\n",
    "#             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:14:29.557419Z",
     "start_time": "2020-07-23T14:14:29.473619Z"
    }
   },
   "outputs": [],
   "source": [
    "user, positive = np.where(np.asanyarray(np.isnan(train_trans[(train_trans == 0)].T)))\n",
    "\n",
    "ind = 0\n",
    "l, users = [[]], [0]\n",
    "for i, j in zip(user, positive):\n",
    "    if i == ind:\n",
    "        l[-1].append(j)\n",
    "    else:\n",
    "        ind = i\n",
    "        users.append(i)\n",
    "        l.append([])\n",
    "        l[-1].append(j)\n",
    "        \n",
    "negative = [[i  for i in range(0, max(positive) + 1) if i not in j] for j in l]\n",
    "positive = l.copy()                \n",
    "    \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:35:02.234519Z",
     "start_time": "2020-07-23T14:27:40.717643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1208e+08, grad_fn=<AddBackward0>)\n",
      "tensor(95162016., grad_fn=<AddBackward0>)\n",
      "tensor(87515072., grad_fn=<AddBackward0>)\n",
      "tensor(84061392., grad_fn=<AddBackward0>)\n",
      "tensor(81223368., grad_fn=<AddBackward0>)\n",
      "tensor(77070920., grad_fn=<AddBackward0>)\n",
      "tensor(70885056., grad_fn=<AddBackward0>)\n",
      "tensor(63242880., grad_fn=<AddBackward0>)\n",
      "tensor(55796904., grad_fn=<AddBackward0>)\n",
      "tensor(50055292., grad_fn=<AddBackward0>)\n",
      "tensor(46579376., grad_fn=<AddBackward0>)\n",
      "tensor(44955040., grad_fn=<AddBackward0>)\n",
      "tensor(44397820., grad_fn=<AddBackward0>)\n",
      "tensor(44403140., grad_fn=<AddBackward0>)\n",
      "tensor(44631648., grad_fn=<AddBackward0>)\n",
      "tensor(44906872., grad_fn=<AddBackward0>)\n",
      "tensor(45195508., grad_fn=<AddBackward0>)\n",
      "tensor(45369376., grad_fn=<AddBackward0>)\n",
      "tensor(45261396., grad_fn=<AddBackward0>)\n",
      "tensor(44922708., grad_fn=<AddBackward0>)\n",
      "tensor(44492144., grad_fn=<AddBackward0>)\n",
      "tensor(44116000., grad_fn=<AddBackward0>)\n",
      "tensor(43929448., grad_fn=<AddBackward0>)\n",
      "tensor(43846780., grad_fn=<AddBackward0>)\n",
      "tensor(43712148., grad_fn=<AddBackward0>)\n",
      "tensor(43613936., grad_fn=<AddBackward0>)\n",
      "tensor(43443640., grad_fn=<AddBackward0>)\n",
      "tensor(43413224., grad_fn=<AddBackward0>)\n",
      "tensor(43399236., grad_fn=<AddBackward0>)\n",
      "tensor(43077320., grad_fn=<AddBackward0>)\n",
      "tensor(42915184., grad_fn=<AddBackward0>)\n",
      "tensor(42604972., grad_fn=<AddBackward0>)\n",
      "tensor(42422508., grad_fn=<AddBackward0>)\n",
      "tensor(42156656., grad_fn=<AddBackward0>)\n",
      "tensor(42192148., grad_fn=<AddBackward0>)\n",
      "tensor(42180752., grad_fn=<AddBackward0>)\n",
      "tensor(42032688., grad_fn=<AddBackward0>)\n",
      "tensor(41779960., grad_fn=<AddBackward0>)\n",
      "tensor(41376568., grad_fn=<AddBackward0>)\n",
      "tensor(41025544., grad_fn=<AddBackward0>)\n",
      "tensor(40808684., grad_fn=<AddBackward0>)\n",
      "tensor(40711576., grad_fn=<AddBackward0>)\n",
      "tensor(40725944., grad_fn=<AddBackward0>)\n",
      "tensor(40550072., grad_fn=<AddBackward0>)\n",
      "tensor(40617808., grad_fn=<AddBackward0>)\n",
      "tensor(40479472., grad_fn=<AddBackward0>)\n",
      "tensor(40549228., grad_fn=<AddBackward0>)\n",
      "tensor(40690900., grad_fn=<AddBackward0>)\n",
      "tensor(40460404., grad_fn=<AddBackward0>)\n",
      "tensor(40397412., grad_fn=<AddBackward0>)\n",
      "tensor(40226604., grad_fn=<AddBackward0>)\n",
      "tensor(40207160., grad_fn=<AddBackward0>)\n",
      "tensor(40543940., grad_fn=<AddBackward0>)\n",
      "tensor(40603104., grad_fn=<AddBackward0>)\n",
      "tensor(40648712., grad_fn=<AddBackward0>)\n",
      "tensor(40740012., grad_fn=<AddBackward0>)\n",
      "tensor(41083816., grad_fn=<AddBackward0>)\n",
      "tensor(40569636., grad_fn=<AddBackward0>)\n",
      "tensor(41147744., grad_fn=<AddBackward0>)\n",
      "tensor(40765468., grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-558-8ef70c886397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_users, n_items = 943, 1683\n",
    "\n",
    "model = BPRloss(n_users, n_items, n_factors=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr= 0.0002, weight_decay = 1e-3 ) # learning rate\n",
    "\n",
    "l = np.array(list(range(943)))\n",
    "\n",
    "test, train = [], []\n",
    "for i in range(100):\n",
    "    #random.shuffle(l)\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    \n",
    "    for u, p, n in zip(users, positive, negative):\n",
    "            # Turn data into tensors\n",
    "            #print(u)\n",
    "            u = Variable(torch.LongTensor([u]))\n",
    "            p = Variable(torch.LongTensor(p))\n",
    "            n = Variable(torch.LongTensor(n))\n",
    "  \n",
    "            # Predict and calculate loss\n",
    "            loss = model(user = u, positive = p, negative = n)\n",
    "            #method1: register and match\n",
    "            #prediction.register_hook(lambda grad: grad * torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values) ))\n",
    "            \n",
    "            #method 1 mask after model\n",
    "#             sign = torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values))\n",
    "#             loss = loss_func(prediction * sign, rating * sign)\n",
    "\n",
    "            #mask in the model\n",
    "#             loss = loss_func(prediction , rating )\n",
    "            \n",
    "                \n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "    print(loss_sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:37:56.705283Z",
     "start_time": "2020-07-23T14:37:56.670670Z"
    }
   },
   "outputs": [],
   "source": [
    "class Hingeloss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items,  n_factors=2):\n",
    "        super().__init__()\n",
    "        self.user_matrix = torch.nn.Embedding(n_users, n_factors)\n",
    "        self.item_matrix = torch.nn.Embedding(n_items, n_factors)\n",
    "        torch.nn.init.normal_(self.user_matrix.weight, 0.001)\n",
    "        torch.nn.init.normal_(self.item_matrix.weight, 0.001)\n",
    "        #torch.nn.init.normal_(self.act.weight, 0.01)\n",
    "        self.m = torch.nn.Sigmoid()\n",
    "    def forward(self, user, positive, negative): #squeeze remove dimension equals to 1\n",
    "        #print(user)\n",
    "        #print(self.user_matrix(user))\n",
    "        um = torch.matmul(self.user_matrix(user), self.item_matrix(positive).T ).reshape(-1) \n",
    "        im = torch.matmul(self.user_matrix(user), self.item_matrix(negative).T ).reshape(-1) \n",
    "        \n",
    "        #print(im)\n",
    "        um_shape = um.shape[0]\n",
    "        im_shape = im.shape[0]\n",
    "        um_m = um.unsqueeze_(-1).expand(um_shape,im_shape)\n",
    "        im_m = im.unsqueeze_(-1).expand(im_shape,um_shape).T\n",
    "        #print(im_m)\n",
    "        zero = torch.zeros(um_shape,im_shape)\n",
    "        #print((um_m  - im_m).reshape(-1))\n",
    "        return torch.max(( - um_m  + im_m + 0.001), zero).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:41:33.693951Z",
     "start_time": "2020-07-23T14:38:22.285448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(62064808., grad_fn=<AddBackward0>)\n",
      "tensor(34834832., grad_fn=<AddBackward0>)\n",
      "tensor(19037854., grad_fn=<AddBackward0>)\n",
      "tensor(12541893., grad_fn=<AddBackward0>)\n",
      "tensor(9895633., grad_fn=<AddBackward0>)\n",
      "tensor(9367830., grad_fn=<AddBackward0>)\n",
      "tensor(10233316., grad_fn=<AddBackward0>)\n",
      "tensor(12042879., grad_fn=<AddBackward0>)\n",
      "tensor(13754000., grad_fn=<AddBackward0>)\n",
      "tensor(15081594., grad_fn=<AddBackward0>)\n",
      "tensor(15164789., grad_fn=<AddBackward0>)\n",
      "tensor(14506167., grad_fn=<AddBackward0>)\n",
      "tensor(14013950., grad_fn=<AddBackward0>)\n",
      "tensor(12526616., grad_fn=<AddBackward0>)\n",
      "tensor(12094333., grad_fn=<AddBackward0>)\n",
      "tensor(12018341., grad_fn=<AddBackward0>)\n",
      "tensor(12463898., grad_fn=<AddBackward0>)\n",
      "tensor(12775337., grad_fn=<AddBackward0>)\n",
      "tensor(13117063., grad_fn=<AddBackward0>)\n",
      "tensor(14441065., grad_fn=<AddBackward0>)\n",
      "tensor(14384508., grad_fn=<AddBackward0>)\n",
      "tensor(14128074., grad_fn=<AddBackward0>)\n",
      "tensor(14170835., grad_fn=<AddBackward0>)\n",
      "tensor(14606866., grad_fn=<AddBackward0>)\n",
      "tensor(14670116., grad_fn=<AddBackward0>)\n",
      "tensor(14761646., grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-563-5c892d533679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_users, n_items = 943, 1683\n",
    "\n",
    "model = Hingeloss(n_users, n_items, n_factors=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr= 0.0002, weight_decay = 1e-3 ) # learning rate\n",
    "\n",
    "l = np.array(list(range(943)))\n",
    "\n",
    "test, train = [], []\n",
    "for i in range(100):\n",
    "    #random.shuffle(l)\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    \n",
    "    for u, p, n in zip(users, positive, negative):\n",
    "            # Turn data into tensors\n",
    "            #print(u)\n",
    "            u = Variable(torch.LongTensor([u]))\n",
    "            p = Variable(torch.LongTensor(p))\n",
    "            n = Variable(torch.LongTensor(n))\n",
    "  \n",
    "            # Predict and calculate loss\n",
    "            loss = model(user = u, positive = p, negative = n)\n",
    "            #method1: register and match\n",
    "            #prediction.register_hook(lambda grad: grad * torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values) ))\n",
    "            \n",
    "            #method 1 mask after model\n",
    "#             sign = torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values))\n",
    "#             loss = loss_func(prediction * sign, rating * sign)\n",
    "\n",
    "            #mask in the model\n",
    "#             loss = loss_func(prediction , rating )\n",
    "            \n",
    "                \n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "    print(loss_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:35:44.187440Z",
     "start_time": "2020-07-23T14:35:44.181641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000],\n",
       "        [0.5342, 0.0798]])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.zeros(2, 2), input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering for Personalized Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T13:25:01.333721Z",
     "start_time": "2020-07-28T13:25:01.246726Z"
    }
   },
   "source": [
    "for implicit feedback\n",
    "contains two subnets (generalized matrix factorization and MLP)\n",
    "GMF: just as before with activation \n",
    "MLP: concatenation of user and item embeddings as input\n",
    "connect second last layers of two subnetworks to create a feature vector and pass it to further layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T14:07:07.411233Z",
     "start_time": "2020-07-28T14:07:07.353408Z"
    }
   },
   "outputs": [],
   "source": [
    "#NeuMF \n",
    "class NeuMF(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_hiddens,  n_factors=20, dropout = 0.05):\n",
    "        super().__init__()\n",
    "        self.P = torch.nn.Embedding(n_users, n_factors)\n",
    "        self.Q = torch.nn.Embedding(n_items, n_factors)\n",
    "        self.U = torch.nn.Embedding(n_users, n_factors)\n",
    "        self.V = torch.nn.Embedding(n_items, n_factors)\n",
    "        \n",
    "        modules = []\n",
    "        for n_input, n_out in zip([2*n_factors] + n_hiddens[:-1], n_hiddens) :\n",
    "            modules.append(torch.nn.Linear(n_input, n_out,\n",
    "                                          bias = True))\n",
    "            modules.append(torch.nn.ReLU())\n",
    "            \n",
    "        self.mlp = torch.nn.Sequential(*modules)\n",
    "        torch.nn.init.normal_(self.P.weight, 0.01)\n",
    "        torch.nn.init.normal_(self.Q.weight, 0.01)\n",
    "        torch.nn.init.normal_(self.U.weight, 0.01)\n",
    "        torch.nn.init.normal_(self.V.weight, 0.01)\n",
    "        #torch.nn.init.normal_(self.act.weight, 0.01)\n",
    "        \n",
    "    def forward(self, user_id, item_id, is_train=1 ): #squeeze remove dimension equals to 1\n",
    "        p_mf = self.P(user_id)\n",
    "        q_mf = self.Q(item_id)\n",
    "        gmf = p_mf * q_mf\n",
    "        p_mlp = self.U(user_id)\n",
    "        q_mlp = self.V(item_id)\n",
    "        mlp = self.mlp(torch.cat((p_mlp, q_mlp), 1))\n",
    "#         if autograd.is_training():  # Mask the gradient during training\n",
    "#             return pred * np.sign(input) \n",
    "#         else:\n",
    "#             \n",
    "        con_res = torch.cat((gmf, mlp), 1).sum(-1)\n",
    "        return con_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T14:31:53.626682Z",
     "start_time": "2020-07-28T14:31:53.619443Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuMF(n_users = 2, n_items = 2, n_hiddens = [4,2])\n",
    "asw = model(Variable(torch.LongTensor([1, 1])),Variable(torch.LongTensor([1,1])))\n",
    "asw2 = model(Variable(torch.LongTensor([1, 1])),Variable(torch.LongTensor([1,1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T14:32:42.933197Z",
     "start_time": "2020-07-28T14:32:42.918477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3863, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BPRLoss(asw, asw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T14:21:49.683782Z",
     "start_time": "2020-07-28T14:21:49.638848Z"
    }
   },
   "outputs": [],
   "source": [
    "#Negative Sampling\n",
    "#positive samples > negative samples (already done)\n",
    "import random \n",
    "n_sample = []\n",
    "for n, p in zip(negative, positive):\n",
    "\n",
    "    n_sample.append(random.choices(n, k = len(p)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T13:46:55.747176Z",
     "start_time": "2020-08-05T13:46:55.741818Z"
    }
   },
   "outputs": [],
   "source": [
    "def BPRLoss(pos, neg):\n",
    "    distances = (pos - neg).reshape(-1)\n",
    "    loss = - torch.log(torch.sigmoid(distances) + 0.0001).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T14:08:57.446343Z",
     "start_time": "2020-08-03T13:33:37.787249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5962866349878219\n",
      "0.5346298743879292\n",
      "tensor(172737.8125, grad_fn=<AddBackward0>)\n",
      "0.6627589677619545\n",
      "0.5573143959597492\n",
      "tensor(84051.7656, grad_fn=<AddBackward0>)\n",
      "0.7033903423385586\n",
      "0.5640069016441562\n",
      "tensor(51402.9766, grad_fn=<AddBackward0>)\n",
      "0.7589282076268971\n",
      "0.591100611330691\n",
      "tensor(37356.6641, grad_fn=<AddBackward0>)\n",
      "0.7830216532356353\n",
      "0.608045149396756\n",
      "tensor(27960.9199, grad_fn=<AddBackward0>)\n",
      "0.7908221744729558\n",
      "0.6201288918755352\n",
      "tensor(24620.4531, grad_fn=<AddBackward0>)\n",
      "0.7797380341339364\n",
      "0.6097911276611745\n",
      "tensor(25137.6074, grad_fn=<AddBackward0>)\n",
      "0.7525808483553155\n",
      "0.5906833934665766\n",
      "tensor(27000.6719, grad_fn=<AddBackward0>)\n",
      "0.7842024919474737\n",
      "0.6187379149969304\n",
      "tensor(30262.8203, grad_fn=<AddBackward0>)\n",
      "0.7772533298475035\n",
      "0.613750638108886\n",
      "tensor(24097.6621, grad_fn=<AddBackward0>)\n",
      "0.7826899534536998\n",
      "0.610564483493078\n",
      "tensor(23099.3770, grad_fn=<AddBackward0>)\n",
      "0.7939555871729562\n",
      "0.6252971697374153\n",
      "tensor(19753.7832, grad_fn=<AddBackward0>)\n",
      "0.7924006562061293\n",
      "0.6126682685760809\n",
      "tensor(18228.0273, grad_fn=<AddBackward0>)\n",
      "0.8005933952232069\n",
      "0.6201687689652944\n",
      "tensor(16969.1758, grad_fn=<AddBackward0>)\n",
      "0.8041044597585869\n",
      "0.6205600028759855\n",
      "tensor(14810.6143, grad_fn=<AddBackward0>)\n",
      "0.8048215376542981\n",
      "0.6215777836347038\n",
      "tensor(13354.9707, grad_fn=<AddBackward0>)\n",
      "0.8099495132182203\n",
      "0.6218924910286532\n",
      "tensor(11073.2207, grad_fn=<AddBackward0>)\n",
      "0.8127754725939637\n",
      "0.6227694930256362\n",
      "tensor(9588.0742, grad_fn=<AddBackward0>)\n",
      "0.8105327615758294\n",
      "0.6184838529715149\n",
      "tensor(8826.4219, grad_fn=<AddBackward0>)\n",
      "0.8144470119061961\n",
      "0.6225080378353821\n",
      "tensor(9058.2061, grad_fn=<AddBackward0>)\n",
      "0.8151445204572647\n",
      "0.6238044387145107\n",
      "tensor(9245.6035, grad_fn=<AddBackward0>)\n",
      "0.8160424542748457\n",
      "0.6196706774631348\n",
      "tensor(7719.7231, grad_fn=<AddBackward0>)\n",
      "0.8169320789729232\n",
      "0.6195084100879729\n",
      "tensor(6497.6836, grad_fn=<AddBackward0>)\n",
      "0.8162509806942\n",
      "0.6172979907462891\n",
      "tensor(5902.5684, grad_fn=<AddBackward0>)\n",
      "0.8175360252277081\n",
      "0.6176951830516639\n",
      "tensor(5959.1714, grad_fn=<AddBackward0>)\n",
      "0.8176628115318624\n",
      "0.6174139313013556\n",
      "tensor(5586.0283, grad_fn=<AddBackward0>)\n",
      "0.8185215652781741\n",
      "0.6177482763597023\n",
      "tensor(5437.8872, grad_fn=<AddBackward0>)\n",
      "0.81790709778016\n",
      "0.6165138593195888\n",
      "tensor(5713.1685, grad_fn=<AddBackward0>)\n",
      "0.8157970557702632\n",
      "0.6146180775034594\n",
      "tensor(5993.8569, grad_fn=<AddBackward0>)\n",
      "0.8179036710275347\n",
      "0.6186958686157383\n",
      "tensor(6235.5391, grad_fn=<AddBackward0>)\n",
      "0.8170404058990586\n",
      "0.6150153634619844\n",
      "tensor(5739.3252, grad_fn=<AddBackward0>)\n",
      "0.8179203031030864\n",
      "0.6151400948246449\n",
      "tensor(5763.6152, grad_fn=<AddBackward0>)\n",
      "0.816344841775004\n",
      "0.6154057674635091\n",
      "tensor(5641.1064, grad_fn=<AddBackward0>)\n",
      "0.8174175390693391\n",
      "0.6142160248600349\n",
      "tensor(5901.1924, grad_fn=<AddBackward0>)\n",
      "0.8174215882020894\n",
      "0.6132703256513545\n",
      "tensor(5908.5552, grad_fn=<AddBackward0>)\n",
      "0.8167557479167852\n",
      "0.6139169015742333\n",
      "tensor(5836.0615, grad_fn=<AddBackward0>)\n",
      "0.8166721665069134\n",
      "0.616253946644599\n",
      "tensor(5771.0483, grad_fn=<AddBackward0>)\n",
      "0.8170217344258065\n",
      "0.6180630698784859\n",
      "tensor(5761.4521, grad_fn=<AddBackward0>)\n",
      "0.8169167725304753\n",
      "0.618879574213405\n",
      "tensor(5873.8599, grad_fn=<AddBackward0>)\n",
      "0.8172889695570325\n",
      "0.6181420819987875\n",
      "tensor(6009.3218, grad_fn=<AddBackward0>)\n",
      "0.8173286458796722\n",
      "0.6175064383719143\n",
      "tensor(6177.1094, grad_fn=<AddBackward0>)\n",
      "0.817308846678438\n",
      "0.6170754162540455\n",
      "tensor(6325.0454, grad_fn=<AddBackward0>)\n",
      "0.8184344245421153\n",
      "0.616612946859131\n",
      "tensor(6499.9326, grad_fn=<AddBackward0>)\n",
      "0.8195333549825471\n",
      "0.6161337134564137\n",
      "tensor(6588.4053, grad_fn=<AddBackward0>)\n",
      "0.8209681957951686\n",
      "0.6159418994788387\n",
      "tensor(6602.0332, grad_fn=<AddBackward0>)\n",
      "0.8213788636101363\n",
      "0.615775443811819\n",
      "tensor(6692.6143, grad_fn=<AddBackward0>)\n",
      "0.8222323884261555\n",
      "0.6159858708548495\n",
      "tensor(6736.4165, grad_fn=<AddBackward0>)\n",
      "0.823768180872248\n",
      "0.6161546798850899\n",
      "tensor(6637.8369, grad_fn=<AddBackward0>)\n",
      "0.8250887215896375\n",
      "0.6174534870835904\n",
      "tensor(6473.3066, grad_fn=<AddBackward0>)\n",
      "0.8253811343300135\n",
      "0.6173156496627447\n",
      "tensor(6353.9678, grad_fn=<AddBackward0>)\n",
      "0.8263355006724282\n",
      "0.6170133631548929\n",
      "tensor(6250.3945, grad_fn=<AddBackward0>)\n",
      "0.8273062775391202\n",
      "0.6185872607066919\n",
      "tensor(6183.5464, grad_fn=<AddBackward0>)\n",
      "0.8267112478652505\n",
      "0.6189136552184178\n",
      "tensor(5997.7290, grad_fn=<AddBackward0>)\n",
      "0.8273787740029815\n",
      "0.6189345404331889\n",
      "tensor(5811.0732, grad_fn=<AddBackward0>)\n",
      "0.828772010991596\n",
      "0.6175449789702346\n",
      "tensor(5590.2144, grad_fn=<AddBackward0>)\n",
      "0.8301636593068469\n",
      "0.6177632802280483\n",
      "tensor(5393.2632, grad_fn=<AddBackward0>)\n",
      "0.8316937635193616\n",
      "0.6182580519472526\n",
      "tensor(5265.6440, grad_fn=<AddBackward0>)\n",
      "0.8329055359334093\n",
      "0.6205758849634028\n",
      "tensor(5154.7393, grad_fn=<AddBackward0>)\n",
      "0.8332660505900358\n",
      "0.620460777185279\n",
      "tensor(5046.8594, grad_fn=<AddBackward0>)\n",
      "0.8337665478137661\n",
      "0.6181283886902383\n",
      "tensor(4898.4131, grad_fn=<AddBackward0>)\n",
      "0.8351355113404421\n",
      "0.6200485879126691\n",
      "tensor(4857.8398, grad_fn=<AddBackward0>)\n",
      "0.8365605150540282\n",
      "0.6206492176351062\n",
      "tensor(4810.9189, grad_fn=<AddBackward0>)\n",
      "0.8368329732947788\n",
      "0.6202199170094265\n",
      "tensor(4793.2695, grad_fn=<AddBackward0>)\n",
      "0.8376881884300837\n",
      "0.6218316177254635\n",
      "tensor(4797.5156, grad_fn=<AddBackward0>)\n",
      "0.8389577643783847\n",
      "0.6220298904665694\n",
      "tensor(4779.4097, grad_fn=<AddBackward0>)\n",
      "0.8396888977978536\n",
      "0.6232905141956703\n",
      "tensor(4758.3037, grad_fn=<AddBackward0>)\n",
      "0.8411332380507425\n",
      "0.6239540119407587\n",
      "tensor(4736.1421, grad_fn=<AddBackward0>)\n",
      "0.8433383364605146\n",
      "0.6236959714484567\n",
      "tensor(4667.3311, grad_fn=<AddBackward0>)\n",
      "0.8450985396370528\n",
      "0.6256589970759587\n",
      "tensor(4612.7578, grad_fn=<AddBackward0>)\n",
      "0.8451407771851868\n",
      "0.6253463971814827\n",
      "tensor(4597.0566, grad_fn=<AddBackward0>)\n",
      "0.8464711293567749\n",
      "0.6237562432884395\n",
      "tensor(4532.7378, grad_fn=<AddBackward0>)\n",
      "0.8476593194549663\n",
      "0.6243535732623122\n",
      "tensor(4450.8389, grad_fn=<AddBackward0>)\n",
      "0.8484494581721497\n",
      "0.6247618229337175\n",
      "tensor(4378.7129, grad_fn=<AddBackward0>)\n",
      "0.8499229893622104\n",
      "0.6258716095014388\n",
      "tensor(4347.6079, grad_fn=<AddBackward0>)\n",
      "0.8506486718377225\n",
      "0.6264058528578869\n",
      "tensor(4315.0405, grad_fn=<AddBackward0>)\n",
      "0.8514267983872921\n",
      "0.6267090210251843\n",
      "tensor(4255.8804, grad_fn=<AddBackward0>)\n",
      "0.8519710275372667\n",
      "0.6239682284390077\n",
      "tensor(4257.7148, grad_fn=<AddBackward0>)\n",
      "0.8528711227381783\n",
      "0.624697447960797\n",
      "tensor(4265.0391, grad_fn=<AddBackward0>)\n",
      "0.8531113560400712\n",
      "0.6246921732187937\n",
      "tensor(4225.2686, grad_fn=<AddBackward0>)\n",
      "0.855005150177733\n",
      "0.6238759914645852\n",
      "tensor(4179.8940, grad_fn=<AddBackward0>)\n",
      "0.8550007671185627\n",
      "0.6219912628486237\n",
      "tensor(4181.9272, grad_fn=<AddBackward0>)\n",
      "0.8561840261999892\n",
      "0.6240500573484163\n",
      "tensor(4139.3398, grad_fn=<AddBackward0>)\n",
      "0.8570224303668261\n",
      "0.6238890550804973\n",
      "tensor(4160.2266, grad_fn=<AddBackward0>)\n",
      "0.8573845194910172\n",
      "0.6225622887098019\n",
      "tensor(4136.5835, grad_fn=<AddBackward0>)\n",
      "0.8582033392868604\n",
      "0.6219204222823159\n",
      "tensor(4100.3882, grad_fn=<AddBackward0>)\n",
      "0.8601672563504219\n",
      "0.6240175160982523\n",
      "tensor(4051.7510, grad_fn=<AddBackward0>)\n",
      "0.8593577248820202\n",
      "0.6262024544832672\n",
      "tensor(4027.3643, grad_fn=<AddBackward0>)\n",
      "0.8605381033864895\n",
      "0.6276802390094032\n",
      "tensor(4070.8794, grad_fn=<AddBackward0>)\n",
      "0.8614589819185067\n",
      "0.6298382851495895\n",
      "tensor(4039.8264, grad_fn=<AddBackward0>)\n",
      "0.8624418105962204\n",
      "0.6321458688719196\n",
      "tensor(4062.8179, grad_fn=<AddBackward0>)\n",
      "0.863903309546198\n",
      "0.6326482495895318\n",
      "tensor(4059.7839, grad_fn=<AddBackward0>)\n",
      "0.8650303610675034\n",
      "0.6320763083888142\n",
      "tensor(4076.8638, grad_fn=<AddBackward0>)\n",
      "0.8654036025819014\n",
      "0.6320261172476161\n",
      "tensor(4055.2429, grad_fn=<AddBackward0>)\n",
      "0.8658210619163076\n",
      "0.6330639038581107\n",
      "tensor(4049.6841, grad_fn=<AddBackward0>)\n",
      "0.8669662766649885\n",
      "0.6345443208737851\n",
      "tensor(3966.2288, grad_fn=<AddBackward0>)\n",
      "0.8670602536657908\n",
      "0.6342656945609872\n",
      "tensor(3927.1460, grad_fn=<AddBackward0>)\n",
      "0.8681681128987088\n",
      "0.6324387552032789\n",
      "tensor(3913.3213, grad_fn=<AddBackward0>)\n",
      "0.868800885326811\n",
      "0.6331066243945713\n",
      "tensor(3938.5613, grad_fn=<AddBackward0>)\n",
      "0.8687962785082821\n",
      "0.6312186387499534\n",
      "tensor(3968.5896, grad_fn=<AddBackward0>)\n",
      "0.870306735447947\n",
      "0.632283281687224\n",
      "tensor(3954.5068, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_users, n_items = 943, 1683\n",
    "\n",
    "model = NeuMF(n_users, n_items, n_hiddens = [8,4,2])\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr= 0.0002, weight_decay = 1e-3 ) # learning rate\n",
    "\n",
    "\n",
    "test, train = [], []\n",
    "for i in range(100):\n",
    "    #random.shuffle(l)\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    \n",
    "    for u, p, n in zip(users, positive, n_sample):\n",
    "            # Turn data into tensors\n",
    "            #print(u)\n",
    "            u = Variable(torch.LongTensor([u] * len(p)))\n",
    "            p = Variable(torch.LongTensor(p))\n",
    "            n = Variable(torch.LongTensor(n))\n",
    "  \n",
    "            # Predict and calculate loss\n",
    "            p_p = model(user_id = u, item_id = p)\n",
    "            n_p = model(user_id = u, item_id = n)\n",
    "            #method1: register and match\n",
    "            #prediction.register_hook(lambda grad: grad * torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values) ))\n",
    "            \n",
    "            #method 1 mask after model\n",
    "#             sign = torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values))\n",
    "#             loss = loss_func(prediction * sign, rating * sign)\n",
    "\n",
    "            #mask in the model\n",
    "#             loss = loss_func(prediction , rating )\n",
    "            \n",
    "            loss = BPRLoss(p_p, n_p)\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            loss_sum += loss\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "    u = Variable(torch.LongTensor(train_u))\n",
    "    p = Variable(torch.LongTensor(train_p))\n",
    "    n = Variable(torch.LongTensor(train_n))\n",
    "    t_p = model(user_id = u, item_id = p)\n",
    "    t_n = model(user_id = u, item_id = n)\n",
    "    #print(t_p.sum()/t_n.sum())\n",
    "    d = {}\n",
    "    for i in t_p:\n",
    "        d[i] = 1\n",
    "    for i in t_n:\n",
    "        d[i] = 0\n",
    "    print(ndcg(d, len(t_p)))\n",
    "    u = Variable(torch.LongTensor(test_u))\n",
    "    p = Variable(torch.LongTensor(test_p))\n",
    "    n = Variable(torch.LongTensor(test_n))\n",
    "    t_p = model(user_id = u, item_id = p)\n",
    "    t_n = model(user_id = u, item_id = n)\n",
    "    d = {}\n",
    "    for i in t_p:\n",
    "        d[i] = 1\n",
    "    for i in t_n:\n",
    "        d[i] = 0\n",
    "    print(ndcg(d, len(t_p)))\n",
    "    print(loss_sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T13:29:11.364042Z",
     "start_time": "2020-08-03T13:29:10.522363Z"
    }
   },
   "outputs": [],
   "source": [
    "#hit rate\n",
    "#recommended item is included in top l ranked list\n",
    "#\n",
    "d = {}\n",
    "for i in t_p:\n",
    "    d[i] = 1\n",
    "for i in t_n:\n",
    "    d[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T13:32:47.659065Z",
     "start_time": "2020-08-03T13:32:36.646701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(-42.3004, grad_fn=<SelectBackward>), tensor(-34.8361, grad_fn=<SelectBackward>), tensor(-34.1264, grad_fn=<SelectBackward>), tensor(-34.0896, grad_fn=<SelectBackward>), tensor(-33.6737, grad_fn=<SelectBackward>), tensor(-33.6737, grad_fn=<SelectBackward>), tensor(-33.6343, grad_fn=<SelectBackward>), tensor(-33.6343, grad_fn=<SelectBackward>), tensor(-33.1665, grad_fn=<SelectBackward>), tensor(-33.1665, grad_fn=<SelectBackward>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17698989379453856"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum([i for i in d if d[i] == 1]), sum([i for i in d if d[i] == 0])\n",
    "ndcg(d, len(t_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T14:44:36.529819Z",
     "start_time": "2020-07-28T14:44:29.509356Z"
    }
   },
   "outputs": [],
   "source": [
    "test_user, test_positive = np.where(np.asanyarray(np.isnan(test_trans[(test_trans == 0)].T)))\n",
    "\n",
    "ind = 0\n",
    "l, test_users = [[]], [0]\n",
    "for i, j in zip(test_user, test_positive):\n",
    "    if i == ind:\n",
    "        l[-1].append(j)\n",
    "    else:\n",
    "        ind = i\n",
    "        test_users.append(i)\n",
    "        l.append([])\n",
    "        l[-1].append(j)\n",
    "        \n",
    "test_negative = [[i  for i in range(0, max(test_positive) + 1) if i not in j] for j in l]\n",
    "test_positive = l.copy()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:40:07.675988Z",
     "start_time": "2020-07-30T13:40:07.562801Z"
    }
   },
   "outputs": [],
   "source": [
    "n_test_sample = []\n",
    "for n, p in zip(test_negative, test_positive):\n",
    "\n",
    "    n_test_sample.append(random.choices(n, k = len(p)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T13:33:18.420039Z",
     "start_time": "2020-08-03T13:33:18.410319Z"
    }
   },
   "outputs": [],
   "source": [
    "def ndcg(d, k):\n",
    "    summ = 0\n",
    "    div = 0\n",
    "    l = sorted(d.keys())[::-1][:k]\n",
    "\n",
    "    for ind, i in enumerate(l):\n",
    "        summ += d[i] / np.log2(2 + ind)\n",
    "        div += 1/ np.log2(2 + ind)\n",
    "    return summ/div\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:03:09.320782Z",
     "start_time": "2020-07-30T14:03:09.306107Z"
    }
   },
   "outputs": [],
   "source": [
    "test_u = []\n",
    "for i in range(len(test_positive)):\n",
    "    test_u += [i]*len(test_positive[i])\n",
    "test_p = [j for i in test_positive for j in i]\n",
    "test_n = [j for i in n_test_sample for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:18:51.209833Z",
     "start_time": "2020-07-30T14:18:51.180251Z"
    }
   },
   "outputs": [],
   "source": [
    "train_u = []\n",
    "for i in range(len(positive)):\n",
    "    train_u += [i]*len(positive[i])\n",
    "train_p = [j for i in positive for j in i]\n",
    "train_n = [j for i in n_sample for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:17:41.572340Z",
     "start_time": "2020-07-30T14:17:41.550694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942]"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-Aware Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutional sequence embedding recommendation\n",
    "adopts cnn capture dynamic pattern influences of users recent activities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "#find out all words\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "#Count those words， 产生一个大字典\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "#每个词出现的频率\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "#可能的拼写错误 只犯一个错\n",
    "def edits1(word):\n",
    "    \n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)] \n",
    "    #缺一个字母 handsome -> hansome\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R] \n",
    "    #两个字母写反了 handsome -> hansdome\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1] \n",
    "    #一个字符写错了 handsome -> hanksome\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    #加了一个字母 handsome -> handdsome\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "#可能的拼写错误 犯了两个上个方程提到的错\n",
    "def edits2(word): \n",
    "    \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "#所有可能替换的单词在字典内\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "\n",
    "#找到所有可能的词汇\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "#通过概率找到最有可能的词汇 （P）\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T14:26:25.018449Z",
     "start_time": "2020-08-03T14:26:25.014086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1, 2), (2, 1))"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(np.array([[1, 2], [2,3]]), 1).shape, np.array([[1], [2]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T14:17:23.406167Z",
     "start_time": "2020-08-05T14:17:23.245121Z"
    }
   },
   "outputs": [],
   "source": [
    "#Caser \n",
    "class Caser(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items,  n_factors=10, \n",
    "                 dropout = 0.05, L = 5, d = 16, d_prime = 4):\n",
    "        super().__init__()\n",
    "        self.P = torch.nn.Embedding(n_users, n_factors)\n",
    "        self.Q = torch.nn.Embedding(n_items, n_factors)\n",
    "        self.d_prime, self.d = d_prime, d\n",
    "        # Vertical convolution layer\n",
    "        self.conv_v = torch.nn.Conv2d(1, d_prime, (L, 1))\n",
    "        # Horizontal convolution layer\n",
    "        h = [i + 1 for i in range(L)]\n",
    "        #self.conv_h, self.max_pool = torch.nn.Sequential(), torch.nn.Sequential()\n",
    "        conv_h, max_pool = [], []\n",
    "        for i in h :\n",
    "            conv_h.append(torch.nn.Conv2d(1, d, (i, n_factors)))\n",
    "            max_pool.append(torch.nn.MaxPool1d(L -i + 1))\n",
    "        self.conv_h = torch.nn.Sequential(*conv_h)\n",
    "        self.max_pool = torch.nn.Sequential(*max_pool)\n",
    "        self.fc1_dim_v, self.fc1_dim_h = d_prime * n_factors, d * len(h)\n",
    "        #self.mlp = torch.nn.Sequential(*modules)\n",
    "        self.fc = torch.nn.Linear(d_prime * n_factors + d * L, n_factors)\n",
    "        self.Q_prime = torch.nn.Embedding(n_items, n_factors * 2)\n",
    "        self.b = torch.nn.Embedding(n_items, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        torch.nn.init.normal_(self.P.weight, 0.01)\n",
    "        torch.nn.init.normal_(self.Q.weight, 0.01)\n",
    "        torch.nn.init.normal_(self.Q_prime.weight, 0.01)\n",
    "        torch.nn.init.normal_(self.b.weight, 0.01)\n",
    "        torch.nn.init.normal_(self.fc.weight, 0.01)\n",
    "        \n",
    "    def forward(self, user_id, seq, item_id, is_train=1 ): #squeeze remove dimension equals to 1\n",
    "        item_embs = torch.unsqueeze(self.Q(seq),1)\n",
    "        user_emb = self.P(user_id)\n",
    "        out, out_h, out_v, out_hs = None, None, None, []\n",
    "        if self.d_prime:\n",
    "            out_v = self.conv_v(item_embs)\n",
    "            out_v = out_v.reshape(out_v.shape[0], self.fc1_dim_v)\n",
    "        if self.d:\n",
    "            for conv, maxp in zip(self.conv_h, self.max_pool):\n",
    "                conv_out = torch.squeeze(self.relu(conv(item_embs)), 3)\n",
    "                t = maxp(conv_out)\n",
    "                pool_out = torch.squeeze(t, 2)\n",
    "                out_hs.append(pool_out)\n",
    "            out_h = torch.cat(out_hs, axis=1)\n",
    "        out = torch.cat((out_h, out_v), axis=1)\n",
    "        z = self.fc(self.dropout(out))\n",
    "        x = torch.cat((z, user_emb), axis=1)\n",
    "        q_prime_i = torch.squeeze(self.Q_prime(item_id))\n",
    "        b = torch.squeeze(self.b(item_id))\n",
    "        res = (x * q_prime_i).sum(1) + b\n",
    "#         if autograd.is_training():  # Mask the gradient during training\n",
    "#             return pred * np.sign(input) \n",
    "#         else:\n",
    "#             \n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T14:01:30.909152Z",
     "start_time": "2020-08-05T14:01:30.899764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-81.4208,   2.1238], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Caser(10, 10)\n",
    "n(torch.LongTensor([0]), torch.LongTensor([[1,2,3,4,5]]), \n",
    "  torch.LongTensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T13:40:40.584599Z",
     "start_time": "2020-08-05T13:40:40.580267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[259]"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(range(1, 944), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T13:41:45.795036Z",
     "start_time": "2020-08-05T13:41:44.463277Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.sort_values(by='timestamp')\n",
    "train, test = [], []\n",
    "for i in range(1, 944):\n",
    "    item = data[data['user_id'] == i].item_id.values\n",
    "    l = len(item)\n",
    "    if l > 5:\n",
    "        for j in range(l-5):\n",
    "            train += [[i, item[j+5],random.sample(range(1, 944), 1)[0], item[j:j+5]]]\n",
    "        test += [[i, [item[l-1]] + random.sample(range(1, 944), 5), item[-6:-1]]] \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T14:49:44.060719Z",
     "start_time": "2020-08-05T14:19:34.963414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5164369034994698\n",
      "tensor(377782.2188, grad_fn=<AddBackward0>)\n",
      "0.5323435843054083\n",
      "tensor(325579.6562, grad_fn=<AddBackward0>)\n",
      "0.545068928950159\n",
      "tensor(287699.3125, grad_fn=<AddBackward0>)\n",
      "0.5737009544008483\n",
      "tensor(254870.3594, grad_fn=<AddBackward0>)\n",
      "0.5811240721102863\n",
      "tensor(223142.7344, grad_fn=<AddBackward0>)\n",
      "0.5970307529162248\n",
      "tensor(191938.1250, grad_fn=<AddBackward0>)\n",
      "0.5864262990455992\n",
      "tensor(162829.6094, grad_fn=<AddBackward0>)\n",
      "0.6214209968186638\n",
      "tensor(140704.6406, grad_fn=<AddBackward0>)\n",
      "0.6394485683987274\n",
      "tensor(123475.2031, grad_fn=<AddBackward0>)\n",
      "0.6521739130434783\n",
      "tensor(110026.4141, grad_fn=<AddBackward0>)\n",
      "0.6638388123011665\n",
      "tensor(100224.6641, grad_fn=<AddBackward0>)\n",
      "0.6829268292682927\n",
      "tensor(92182.2422, grad_fn=<AddBackward0>)\n",
      "0.6935312831389183\n",
      "tensor(85184.6641, grad_fn=<AddBackward0>)\n",
      "0.6903499469777307\n",
      "tensor(79352.3203, grad_fn=<AddBackward0>)\n",
      "0.6914103923647932\n",
      "tensor(74365.3359, grad_fn=<AddBackward0>)\n",
      "0.7083775185577943\n",
      "tensor(69511.3672, grad_fn=<AddBackward0>)\n",
      "0.6998939554612937\n",
      "tensor(65592.8828, grad_fn=<AddBackward0>)\n",
      "0.6956521739130435\n",
      "tensor(62516.9844, grad_fn=<AddBackward0>)\n",
      "0.7285259809119831\n",
      "tensor(59898.6445, grad_fn=<AddBackward0>)\n",
      "0.7285259809119831\n",
      "tensor(57607.2031, grad_fn=<AddBackward0>)\n",
      "0.7306468716861082\n",
      "tensor(55080.0469, grad_fn=<AddBackward0>)\n",
      "0.7158006362672322\n",
      "tensor(53337.5312, grad_fn=<AddBackward0>)\n",
      "0.7486744432661718\n",
      "tensor(51646.7148, grad_fn=<AddBackward0>)\n",
      "0.7497348886532343\n",
      "tensor(50103.3203, grad_fn=<AddBackward0>)\n",
      "0.7391304347826086\n",
      "tensor(49251.6445, grad_fn=<AddBackward0>)\n",
      "0.7539766702014846\n",
      "tensor(47536.1055, grad_fn=<AddBackward0>)\n",
      "0.7613997879109226\n",
      "tensor(46695.9453, grad_fn=<AddBackward0>)\n",
      "0.7454931071049841\n",
      "tensor(45832.1445, grad_fn=<AddBackward0>)\n",
      "0.7741251325556734\n",
      "tensor(44717.4531, grad_fn=<AddBackward0>)\n",
      "0.7560975609756098\n",
      "tensor(44134.0938, grad_fn=<AddBackward0>)\n",
      "0.7794273594909862\n",
      "tensor(42540.9492, grad_fn=<AddBackward0>)\n",
      "0.7582184517497349\n",
      "tensor(42391.0625, grad_fn=<AddBackward0>)\n",
      "0.7815482502651113\n",
      "tensor(41836.7852, grad_fn=<AddBackward0>)\n",
      "0.7804878048780488\n",
      "tensor(40652.0586, grad_fn=<AddBackward0>)\n",
      "0.7783669141039237\n",
      "tensor(40525.3281, grad_fn=<AddBackward0>)\n",
      "0.7635206786850477\n",
      "tensor(39984.2617, grad_fn=<AddBackward0>)\n",
      "0.7698833510074231\n",
      "tensor(39537.8125, grad_fn=<AddBackward0>)\n",
      "0.7815482502651113\n",
      "tensor(38733.9805, grad_fn=<AddBackward0>)\n",
      "0.777306468716861\n",
      "tensor(39221.7617, grad_fn=<AddBackward0>)\n",
      "0.7635206786850477\n",
      "tensor(39223.5156, grad_fn=<AddBackward0>)\n",
      "0.7730646871686108\n",
      "tensor(38751.5859, grad_fn=<AddBackward0>)\n",
      "0.7836691410392365\n",
      "tensor(38473.4375, grad_fn=<AddBackward0>)\n",
      "0.7762460233297985\n",
      "tensor(38276.8516, grad_fn=<AddBackward0>)\n",
      "0.7762460233297985\n",
      "tensor(37517.2070, grad_fn=<AddBackward0>)\n",
      "0.7741251325556734\n",
      "tensor(37809.1328, grad_fn=<AddBackward0>)\n",
      "0.767762460233298\n",
      "tensor(37707.0117, grad_fn=<AddBackward0>)\n",
      "0.7804878048780488\n",
      "tensor(37305.6406, grad_fn=<AddBackward0>)\n",
      "0.7635206786850477\n",
      "tensor(37160.0820, grad_fn=<AddBackward0>)\n",
      "0.7667020148462355\n",
      "tensor(37266.6211, grad_fn=<AddBackward0>)\n",
      "0.7794273594909862\n",
      "tensor(36330.3672, grad_fn=<AddBackward0>)\n",
      "0.7688229056203606\n",
      "tensor(36571.8125, grad_fn=<AddBackward0>)\n",
      "0.7783669141039237\n",
      "tensor(36136.6250, grad_fn=<AddBackward0>)\n",
      "0.767762460233298\n",
      "tensor(35382.1719, grad_fn=<AddBackward0>)\n",
      "0.7730646871686108\n",
      "tensor(35598.6523, grad_fn=<AddBackward0>)\n",
      "0.777306468716861\n",
      "tensor(35851.5820, grad_fn=<AddBackward0>)\n",
      "0.7656415694591728\n",
      "tensor(35300.2148, grad_fn=<AddBackward0>)\n",
      "0.7804878048780488\n",
      "tensor(35611.0117, grad_fn=<AddBackward0>)\n",
      "0.7635206786850477\n",
      "tensor(37062.9219, grad_fn=<AddBackward0>)\n",
      "0.7751855779427359\n",
      "tensor(38140.0352, grad_fn=<AddBackward0>)\n",
      "0.777306468716861\n",
      "tensor(36461.6133, grad_fn=<AddBackward0>)\n",
      "0.7762460233297985\n",
      "tensor(35910.7773, grad_fn=<AddBackward0>)\n",
      "0.7582184517497349\n",
      "tensor(36362.8242, grad_fn=<AddBackward0>)\n",
      "0.7582184517497349\n",
      "tensor(37178.0977, grad_fn=<AddBackward0>)\n",
      "0.7857900318133616\n",
      "tensor(38607.4062, grad_fn=<AddBackward0>)\n",
      "0.7963944856839873\n",
      "tensor(43984.8945, grad_fn=<AddBackward0>)\n",
      "0.7476139978791092\n",
      "tensor(45489.3789, grad_fn=<AddBackward0>)\n",
      "0.7645811240721103\n",
      "tensor(40855.7695, grad_fn=<AddBackward0>)\n",
      "0.777306468716861\n",
      "tensor(39814.4883, grad_fn=<AddBackward0>)\n",
      "0.7518557794273595\n",
      "tensor(41990.1719, grad_fn=<AddBackward0>)\n",
      "0.7751855779427359\n",
      "tensor(45946.8008, grad_fn=<AddBackward0>)\n",
      "0.7889713679745494\n",
      "tensor(49891.7891, grad_fn=<AddBackward0>)\n",
      "0.7741251325556734\n",
      "tensor(49361.5117, grad_fn=<AddBackward0>)\n",
      "0.7836691410392365\n",
      "tensor(44492.4180, grad_fn=<AddBackward0>)\n",
      "0.7932131495227995\n",
      "tensor(44983.5000, grad_fn=<AddBackward0>)\n",
      "0.7847295864262991\n",
      "tensor(41077.3320, grad_fn=<AddBackward0>)\n",
      "0.7953340402969247\n",
      "tensor(42233.9102, grad_fn=<AddBackward0>)\n",
      "0.7751855779427359\n",
      "tensor(47394.6406, grad_fn=<AddBackward0>)\n",
      "0.750795334040297\n",
      "tensor(47584.3477, grad_fn=<AddBackward0>)\n",
      "0.7974549310710498\n",
      "tensor(52025.0547, grad_fn=<AddBackward0>)\n",
      "0.7963944856839873\n",
      "tensor(50049.4336, grad_fn=<AddBackward0>)\n",
      "0.7539766702014846\n",
      "tensor(50774.1875, grad_fn=<AddBackward0>)\n",
      "0.8006362672322376\n",
      "tensor(47738.4141, grad_fn=<AddBackward0>)\n",
      "0.7868504772004242\n",
      "tensor(45239.5352, grad_fn=<AddBackward0>)\n",
      "0.8006362672322376\n",
      "tensor(44256.5352, grad_fn=<AddBackward0>)\n",
      "0.782608695652174\n",
      "tensor(43046.0859, grad_fn=<AddBackward0>)\n",
      "0.7985153764581124\n",
      "tensor(41563.0508, grad_fn=<AddBackward0>)\n",
      "0.8016967126193001\n",
      "tensor(40582.3086, grad_fn=<AddBackward0>)\n",
      "0.7963944856839873\n",
      "tensor(44705.4570, grad_fn=<AddBackward0>)\n",
      "0.7836691410392365\n",
      "tensor(43894.6367, grad_fn=<AddBackward0>)\n",
      "0.767762460233298\n",
      "tensor(44784.8438, grad_fn=<AddBackward0>)\n",
      "0.8123011664899258\n",
      "tensor(51332.0586, grad_fn=<AddBackward0>)\n",
      "0.8123011664899258\n",
      "tensor(49789.8633, grad_fn=<AddBackward0>)\n",
      "0.792152704135737\n",
      "tensor(51655.8672, grad_fn=<AddBackward0>)\n",
      "0.8197242841993637\n",
      "tensor(46833.6797, grad_fn=<AddBackward0>)\n",
      "0.7942735949098622\n",
      "tensor(44210.3359, grad_fn=<AddBackward0>)\n",
      "0.823966065747614\n",
      "tensor(45700.4609, grad_fn=<AddBackward0>)\n",
      "0.7985153764581124\n",
      "tensor(45641.4492, grad_fn=<AddBackward0>)\n",
      "0.7963944856839873\n",
      "tensor(47176.0312, grad_fn=<AddBackward0>)\n",
      "0.8356309650053022\n",
      "tensor(44167.1445, grad_fn=<AddBackward0>)\n",
      "0.8398727465535525\n",
      "tensor(47924.3867, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_users, n_items = 944, 1683\n",
    "\n",
    "model = Caser(n_users, n_items)\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr= 0.0001, weight_decay = 1e-5 ) # learning rate\n",
    "\n",
    "\n",
    "l = len(train)\n",
    "for i in range(100):\n",
    "    random.shuffle(train)\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    \n",
    "    for j in range(0, l, 256):\n",
    "            # Turn data into tensors\n",
    "            #print(u)\n",
    "\n",
    "            u = Variable(torch.LongTensor([k[0] for k in train[j:j+256]]))\n",
    "            seq = Variable(torch.LongTensor([k[3] for k in train[j:j+256]]))\n",
    "            i = Variable(torch.LongTensor([k[1] for k in train[j:j+256]]))\n",
    "            c = Variable(torch.LongTensor([k[2] for k in train[j:j+256]]))\n",
    "  \n",
    "            # Predict and calculate loss\n",
    "            p_p = model(user_id = u, seq = seq, item_id = i)\n",
    "            n_p = model(user_id = u,  seq = seq, item_id = c)\n",
    "            #method1: register and match\n",
    "            #prediction.register_hook(lambda grad: grad * torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values) ))\n",
    "            \n",
    "            #method 1 mask after model\n",
    "#             sign = torch.FloatTensor(np.sign(train_trans.iloc[l[ind: ind + 256]].values))\n",
    "#             loss = loss_func(prediction * sign, rating * sign)\n",
    "\n",
    "            #mask in the model\n",
    "#             loss = loss_func(prediction , rating )\n",
    "            #print(p_p, n_p)\n",
    "            loss = BPRLoss(p_p, n_p)\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            #print(loss)\n",
    "            loss_sum += loss\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "    u = Variable(torch.LongTensor([k[0] for k in test]))\n",
    "    seq = Variable(torch.LongTensor([k[2] for k in test]))\n",
    "    it = Variable(torch.LongTensor([k[1][0] for k in test]))\n",
    "    tes1 = Variable(torch.LongTensor([k[1][1] for k in test]))\n",
    "    summ = np.sum(model(user_id = u, seq = seq, item_id = it).detach().numpy() >  model(user_id = u, seq = seq, item_id = tes1).detach().numpy())\n",
    "    print(summ/943)\n",
    "#     u = Variable(torch.LongTensor(train_u))\n",
    "#     p = Variable(torch.LongTensor(train_p))\n",
    "#     n = Variable(torch.LongTensor(train_n))\n",
    "#     t_p = model(user_id = u, item_id = p)\n",
    "#     t_n = model(user_id = u, item_id = n)\n",
    "#     #print(t_p.sum()/t_n.sum())\n",
    "#     d = {}\n",
    "#     for i in t_p:\n",
    "#         d[i] = 1\n",
    "#     for i in t_n:\n",
    "#         d[i] = 0\n",
    "#     print(ndcg(d, len(t_p)))\n",
    "#     u = Variable(torch.LongTensor(test_u))\n",
    "#     p = Variable(torch.LongTensor(test_p))\n",
    "#     n = Variable(torch.LongTensor(test_n))\n",
    "#     t_p = model(user_id = u, item_id = p)\n",
    "#     t_n = model(user_id = u, item_id = n)\n",
    "#     d = {}\n",
    "#     for i in t_p:\n",
    "#         d[i] = 1\n",
    "#     for i in t_n:\n",
    "#         d[i] = 0\n",
    "#     print(ndcg(d, len(t_p)))\n",
    "    print(loss_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T14:05:33.868664Z",
     "start_time": "2020-08-05T14:05:33.860606Z"
    }
   },
   "outputs": [],
   "source": [
    "u = Variable(torch.LongTensor([k[0] for k in test]))\n",
    "seq = Variable(torch.LongTensor([k[2] for k in test]))\n",
    "it = Variable(torch.LongTensor([k[1][0] for k in test]))\n",
    "tes1 = Variable(torch.LongTensor([k[1][1] for k in test]))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T14:08:23.907491Z",
     "start_time": "2020-08-05T14:08:23.885144Z"
    }
   },
   "source": [
    "\n",
    "# Feature-Rich Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T14:03:07.637153Z",
     "start_time": "2020-08-05T14:03:07.632716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 74, 281])"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#task \n",
    "#data loading and data integreating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T14:02:44.667634Z",
     "start_time": "2020-08-05T14:02:44.663328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T14:02:54.109182Z",
     "start_time": "2020-08-05T14:02:54.104965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[111, 171,   5, 256, 102],\n",
       "        [289, 316, 308, 309, 314]])"
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T03:00:29.734029Z",
     "start_time": "2020-08-09T03:00:29.619216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for j in set({3,1,3}):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T15:57:18.023455Z",
     "start_time": "2020-08-09T15:57:18.014821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + (1 << 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T15:58:36.207807Z",
     "start_time": "2020-08-09T15:58:36.202987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ^ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strength:\n",
    "    model variable interactions\n",
    "    fast optimization to reduce polynomial computation to linear complexity\n",
    "y(x) = w_0 + \\sum^{d} w_i x_i + \\sum^{d} \\sum_^{d} <v_i, v_j>x_i, x_j v is feature embedding\n",
    "To do:\n",
    "    Deep factorization model\n",
    "    SigmoidBinaryCrossEntropyLoss \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Factorization Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepFM = FM + deep component in a parallel structure\n",
    "FM: 2-way factorization machines \n",
    "Deep component: multi-layer perceptron used to capture high-order feature interactions\n",
    "advantage: reduce the effort of hand-crafted feeture engineering by identifying feature combinations automatically\n",
    "\n",
    "Task: \n",
    "    Modeling\n",
    "    training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
